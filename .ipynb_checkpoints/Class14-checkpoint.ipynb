{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ace1c83-8620-4892-961e-097286ba0cf4",
   "metadata": {},
   "source": [
    "# Random Forest, XGBoost, LightGBM\n",
    "\n",
    "### Emsemble:\n",
    "Muchas veces un solo modelo no consigue la precisión adecuada. Un Ensemble es una combinación de modelos trabajando en conjunto\n",
    "\n",
    "La clave está en COMO combinar las decisiones parciales de esos modelos\n",
    "\n",
    "## 1. Bagging:\n",
    "**CLAVE EXAMEN**\n",
    "* Bootstrap aggregating\n",
    "* Permite samplear o muestrear instancias muchas veces en el mismo predictor\n",
    "* Serían muestreos aleatorios CON REEMPLAZO (bootstrap. Se puede seleccionar el mismo valor en casa muestra) del dataset para trabajar con ellos en cada modelo\n",
    "* La idea es reducir la varianza\n",
    "\n",
    "Tipos de combinaciones de decisiones:\n",
    "* Voto comunitario --> Todos los votos tienen el mismo peso y va por conteo la decisión. Solo se toma en cuenta la clasificación, por ejemplo 0 o 1, no la proba por ahora\n",
    "\n",
    "## Random Forest\n",
    "El Tree normal es INESTABLE, es muy sensible a los datos de entrada!\n",
    "\n",
    "La solución que se busca es NO depender de un único tree!\n",
    "\n",
    "El Random Forest es un método de Bagging\n",
    "\n",
    "Un modelo tonto puede tener 500 trees por ejemplo...\n",
    "\n",
    "### Hiperparámetros\n",
    "Mientras más grandes puedan ser más puede desifrar pero puedo generar mucho overfitting!\n",
    "\n",
    "* Q de trees\n",
    "* Max Depth\n",
    "* etc...\n",
    "\n",
    "## 2. Boosting (gradient boosting)\n",
    "Se le da un peso a cada voto (de cada tree)! Pero la idea es que sea lo más automático posible... \n",
    "\n",
    "El Gradient boosting combina el descenso del gradiente con el boosting...\n",
    "\n",
    "También tiene muestreo aleatorio con reemplazamiento PERO OJO, TIENE TRUCO --> **El propio algorítmo se prueba a si mismo, se da cuenta de lo que falla y entrena más esos datos. Mientras más fallan esas instancias en la clasificación, más probabilidades tienen de ser cojidos!**\n",
    "\n",
    "Que tenga más probabilidad no quiere decir que SIEMPRE sea seleccionado. Sigue siendo un muestreo ALEATORIO, pero ahora con diferentes distribuciones de probabilidad por instancias!\n",
    "\n",
    "Como es **secuencial**, suele tardar más que el Random Forest\n",
    "\n",
    "Combinación de decisiones:\n",
    "* No es voto comunitario: Los votos tienen un determinado peso! --> La decisión final es el sumatorio PONDERADO de las decisiones individuales\n",
    "    - El que más acierta, más peso tendrá!\n",
    "\n",
    "# LightGBM --> USAR Y EXPLICAR EN LA PRÁCTICA!\n",
    "\n",
    "Estos modelos tienen más precisión pero también tiene mayor riesgo de overfitting...\n",
    "\n",
    "\n",
    "* Entonces ver, GBM requiere más computo y tinee más riesgo de overfitting... Pero es mejor x lo gral\n",
    "\n",
    "Que tenga poca varianza significa q mis resultados van a ser muy similares\n",
    "\n",
    "MUCHA VARIANZA: PEQUEÑOS CAMBIOS EN EL DATASET DE ENTRADA PROVOCAN MUCHOS CAMBIOS EN LA SOLUCIÓN. Poca varianza a la inversa\n",
    "\n",
    "El Random Forest tiene MENOR varianza x lo gral que los Gradient Boostings!\n",
    "\n",
    "## HAY Q SABER MEDIR ESTO\n",
    "\n",
    "\n",
    "# Ver Slides: SIMILITUDES Y DIFERENCIAS!\n",
    "\n",
    "***\n",
    "\n",
    "# Regularización --> Controlar la complejidad del modelo para lidiar con el Overfitting...\n",
    "PENALIZAMOS la complejidad del modelo! Lo intentamos limitar sin prohibirlo x ej\n",
    "\n",
    "Penalizamos que crezca mucho hacia abajo el árbol por ejemplo!\n",
    "\n",
    "Como penalizamos, la ganancia que da crecer tanto tiene que superar la penalización --> LIMITAMOS EL OVERFITTING\n",
    "\n",
    "SOLO SE COMPLEJIZA EL MODELO CUANDO GENERA UNA PRECISIÓN SUFICIENTE COMO PARA CORRER RIESGO DE OVERFITTING DIGAMOS\n",
    "\n",
    "Los hiperparámetros como max-depth podrían ser de regularización!\n",
    "\n",
    "En funciones de coste se usa tmb esto!\n",
    "\n",
    "Es un concepto importante!!!\n",
    "\n",
    "***\n",
    "# Hiperparámetros --> INVESTIGAR!\n",
    "Guía:\n",
    "\n",
    "***\n",
    "***\n",
    "# EXPLICABILIDAD (Diferente a INTERPRETABILIDAD!):\n",
    "\n",
    "* Un modelo LINEAL es **INTERPRETABLE**. Por ej, si cambio un peso de una variable, se en cuanto cambiará la Y\n",
    "* Lo del peso de las variables con los trees por ejemplo, es EXPLICABLE, no INTERPRETABLE. Esto es debido a que no se EXACTAMENTE cuanto va a cambiar o no un cambio en una u otra...\n",
    "* En el primero se bien la relación con el output (y), mientras que con el segudno solo se el peso de las variables y la relación con el modelo pero no puedo calcular el output de manera exacta fácilmente sin simular el modelo...\n",
    "\n",
    "\n",
    "Como el random forest decide la importancia de las variables?\n",
    "\n",
    "* Cuando una variable está en más RAMAS, y mientras MÁS ARRIBA aparezca x lo gral (mientras más arriba más importante, más discrimina esa variable)!\n",
    "* Eso determina el valor que tiene, mientras más aparezca y sobre todo mientras MÁS ARRIBA EN EL TREE LO HAGA, más importante será pq más discriminará!\n",
    "\n",
    "## 2 tipos de Explicabilidad!\n",
    "\n",
    "* Global (la q vimos arriba): Lo que se explica del MODELO EN GENERAL\n",
    "* Local (no pueden sacar el RF y el XGB x ej): Cuando el sujeto específico es analizado. La idea es ver la diferencia de una instancia con respecto a las que tiene al rededor y más cercanas, con el fin de ver que cambios en esa instancia la hicieron diferenciarse de las otras...\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "# PRÁCTICA:\n",
    "\n",
    "# APARADO DE BAGGING CON RANDOM FOREST\n",
    "## COMENTAR OTRA OPCIÓN DE BAGGING!!! Y diferencia con el RF!\n",
    "## MÉTRICAS\n",
    "## PLOT IMPORTANCE DE LAS VARIABLES! --> COMENTARLO!!! Decir variables importantes, si tiene sentido o no, si se condice con el EDA y como, etc!\n",
    "\n",
    "# Realizar XGBoost Y LIGHTGBM !!! Son de los más usados\n",
    "## Explicar diferencia entre ambos!!!\n",
    "# %% TIME EN TODO!!!\n",
    "\n",
    "***\n",
    "## Regresión con Emsemblers!\n",
    "* RandomForestRegressor en SKL\n",
    "* XGBRegression\n",
    "* etc...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c223188-e19a-41ff-a524-1c7e25c90390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59214f-8dce-4419-84d2-f8f48ea8a839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29514f53-3bc4-4ff8-8372-fb4f9f4738c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a350088-31e9-4d5d-a109-3712951e1d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c3f3c-2a7e-4750-b8e4-074400227666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa3d4f-77e0-44bf-b265-64796c5cc79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
