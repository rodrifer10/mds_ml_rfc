{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecae84a-6757-444c-ab97-b39e9fb130eb",
   "metadata": {},
   "source": [
    "# Clase 10: Scaling y FEATURING SELECTION\n",
    "\n",
    "# 1- Scaling\n",
    "\n",
    "Hay modelos en los que las variables no pueden tener escalas muy diferentes!\n",
    "\n",
    "Por ejemplo, muchas veces es necesario que tengan la misma media y desvest, **COMO EN LA REGRESIÓN LINEAL Y LOGÍSTICA!**\n",
    "\n",
    "Entonces, **Siempre que hagamos una regresión, hay que escalar las variables! NORMALIZANDO**. En Regresión es así, por ej en decision trees no...\n",
    "\n",
    "* Normalization: --> todas las variables quedan entre 0 y 1 --> media 0 y desvest 1 --> x-min(x) / range\n",
    "* Standarization: --> misma media y misma varianza pero NO entre 0 y 1! --> valor-media / desvest\n",
    "\n",
    "# 2- Métodos de selección de Variables:\n",
    "\n",
    "* Selección de variables con algoritmos: Lo que veremos ahora\n",
    "* Selección de variables\n",
    "\n",
    "#### PUEDO MEZCLAR MÉTODOS. Reducir dimensiones de algunas variables, otras no y así...\n",
    "\n",
    "## Featuring Selection con algorítmos\n",
    "\n",
    "1. Intrinsec del modelo --> Trees\n",
    "2. Aplicar algoritmos recursivamente --> RFE, Soruta\n",
    "3. Filter Methods --> Stats / Feature Importance\n",
    "\n",
    "### 1. Selección de variables con Decision Trees (el que más se usa)\n",
    "\n",
    "Los Trees seleccionan las variables que aportan MÁS INFORMACION\n",
    "\n",
    "Se puede realizar un modelo con todas las variables y luego filtrar el dataset por las que se usen o sean más importantes\n",
    "\n",
    "**OJO, HAY 2 MÉTODOS**, con dos parámetros:\n",
    "1. **Por Ganancia --> ESTE ES EL QUE VALE** --> Mide cuanto de importante es el corte que ha hecho **(method=gain), POR DEFECTO VIENE EL SPLIT, CAMBIAR!**\n",
    "2. Por Split *(method=split)* --> no sirve mucho pq las categóricas salen perdiendo x lo gral\n",
    "\n",
    "SIEMPRE HAY QUE HACER EL GRÁFICO DE IMPORTANCIA DE VARIABLES DSPS DE UN ARBOL DE DECISION!!!\n",
    "\n",
    "* Una vez los tengo, puedo ir probando ejecutar el modelo SACANDO las últimas...\n",
    "* Si voy viendo que empeora la ROC y la Confusion Matrix, puedo ir sacando y probando Y CORRER DE NUEVO EL DECISION TREE AVER QUE SALE!\n",
    "\n",
    "\n",
    "**OJO, LA CURVA ROC ES SENSIBLE A PROBLEMAS NO BALANCEADOS! --> HAY QUE USAR PRECISION Y RECALL(SENSITIVIDAD) PARA COMPENSAR** \n",
    "\n",
    "### 2. Filter Methods: Interpretación de los coeficientes de una regresión!\n",
    "\n",
    "Los coeficientes **indican la importancia de las variables!** --> Si es alto (negativo o positivo) el coeficiente, puedo ver la importancia!\n",
    "\n",
    "**OJO, antes de esto, recordar que hay que ESCALAR!** --> Sino la escala va a afectar en los coeficientes!!! (Recordar que esto es una regresión! Ya sea logística o lineal!)\n",
    "\n",
    "#### Regresión con penalización L1 y L2:\n",
    "\n",
    "Recordar que la COST FUNCTION en una regresión busca disminuír el error cuadrado (MSE), optimizando los mejores coeficientes\n",
    "\n",
    "Estas regresiones suman un termino independiente\n",
    "\n",
    "* Función de coste con **Penalización Ridge(L2)** --> La regresión Ridge PENALIZA en que\n",
    "    - LAMBDA (parámetro) por los coeficientes(w) elevados al cuadrado! --> Hace que si un coeficiente(w) es grande, osea IMPORTANTE, las resalta! Y si es chico, lo acerca a 0! Osea, se carga las variables poco importantes y le da más preponderancia a las más importantes --> **Esto optimiza, pq usa las variables más importantes y las que no, las acerca a 0**\n",
    "* Función de coste con **Penalización Lasso(L1)** --> Es igual que la Ridge pero en valor aboluto en vez de al cuadrado!\n",
    "\n",
    "La diferencia: El Ridge los deja en 0 pero el Lasso no, los deja muy cercano --> Pero en SKLearn Ridge deja muy cercanos a 0 tmb, son parecidos\n",
    "\n",
    "##### Sklearn SelectFeatureImportance\n",
    "\n",
    "Se escogen los coeficientes de las variables con un valor mayor a un **threshold** especificado en la función\n",
    "\n",
    "Es necesario seleccionar un threshold para filtrar valores!! --> **Ver gráficos y valores para decidir a partir de que Threshold eliminamos variables y de cual nos quedamos**\n",
    "\n",
    "(VER SLIDES)\n",
    "\n",
    "## 3. SKLearn RFE (Recursive Feature Elimination)\n",
    "\n",
    "Se puede aplicar prácticamente a cualquier modelo!\n",
    "\n",
    "Selecciona lotes de variable recursivamente en cualquier modelo que aplico!\n",
    "\n",
    "* Va probando modelos con X variables y va eligiendo las más y menos importantes! Todo esto en muchas iteraciones del modelo...\n",
    "* Es pesado computacionamente\n",
    "* TENGO QUE DEFINIR UN NUMERO X DE VARIABLES CON LAS QUE ME QUIERO QUEDAR! --> Tengo que tener alguna idea previa...\n",
    "\n",
    "## 4. Boruta\n",
    "\n",
    "Descoloco valores de una variable de manera random (hace un Shuffle digamos), y si NO hubo muchos cambios en el modelo, significa que NO es importante\n",
    "\n",
    "Lo mismo viceversa... Así veo cuando una variable es importante y cual no\n",
    "\n",
    "El output del modelo es un listado de las variables que son importantes\n",
    "\n",
    "* Tiene una complejidad computacional muy alta, eso si\n",
    "* Se aplica más en Trees que en Regresiones\n",
    "\n",
    "\n",
    "\n",
    "# ¿Como sabemos que la selección de variables es apropiada?\n",
    "\n",
    "* Debe reducir el tiempo de procesamiento\n",
    "* Revisar los Scores!!! --> Si bajan, quitar alguna/s variable/s no estuvo bien!\n",
    "* A un Random Forest o a un XGBoost le puedo aplicar el gain y revisar si funciona el modelo. Pq los arboles NO penalizan con muchas variables. **Este es el principal a usar seguramente!**\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "# En Python:\n",
    "\n",
    "**Ver 04_Vars_Selection.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57a29e-8e3f-4acb-b57e-69d55953f30c",
   "metadata": {},
   "source": [
    "##### Estos métodos se hacen ANTES del modelo, quito variables, voy y pruebo el modelo Y VUELVO acá si no me funcionó del todo\n",
    "##### Voy a ir yendo y viniendo todo el tiempo!\n",
    "##### Lo mismo con los Encodings!! Los puedo ir cambiando y viendo con prueba y error!\n",
    "\n",
    "## Regulariz Ridge:\n",
    "\n",
    "``` Python\n",
    "sel_ridge = SelectFromModel(LogisticRegression(C=1, penalty='l2'), threshold = 1e-4)\n",
    "    # Acá lo que hago es es ponerle la penalización Ridge con el penalty='l2'. C=1 es un término que multiplica al lambda y le da más importancia o menos a la regularización\n",
    "    # El thresthold = 1e-4, que va en el SelectFromModel lo que hace es setear el límite bajo el cual NO use las variables, osea q no le de importancia...\n",
    "    # Tengo que ir probando con el Threshold! Me va a ir cambiando\n",
    "\n",
    "sel_ridge.fit(X_train_scaled, y_train)\n",
    "    # Le paso el X ESCALADO! y el y_train!\n",
    "# Si aplicas transform(X_train_scaled, y_train) y transform(X_test_scaled, y_test), filtra las variables seleccionadas \n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "``` Python\n",
    "# Saco los coeficientes del modelo!\n",
    "# ==============================================================================\n",
    "df_coeficientes = pd.DataFrame(\n",
    "                        {'predictor': X_train_scaled.columns,\n",
    "                         'coef': sel_ridge.estimator_.coef_.flatten()} # con sel_ridge.estimator_.coef_.flatten() saco los coeficientes(w) y los hago una lista plana!\n",
    "                                                                       # El .coef NO es de la regre logística, sino del SelectFromModel!\n",
    "                  )\n",
    "\n",
    "# Grafico:\n",
    "fig, ax = plt.subplots(figsize=(16, 3.84))\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=90, ha='right', size=10)\n",
    "ax.set_xlabel('variable')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.set_title('Coeficientes del modelo ridge');\n",
    "```\n",
    "\n",
    "``` Python\n",
    "# Me quedo todas las del SelectFromModel\n",
    "sel_ridge.get_support()\n",
    "selected_feat = X_train_scaled.columns[sel_ridge.get_support()] # get_support() me debe dar la posición en la lista y yo lo saco de las columns!\n",
    "selected_feat\n",
    "```\n",
    "\n",
    "***\n",
    "## Regulariz tipo Lasso:\n",
    "\n",
    "```Python\n",
    "\n",
    "sel_lasso = SelectFromModel(LogisticRegression(C=1, penalty='l1', \n",
    "                                          solver='liblinear'), threshold = 0.08) # jugar con el threshold!\n",
    "sel_lasso.fit(X_train_scaled, y_train)\n",
    "```\n",
    "Este código es similar al Ridge de arriba, ver en 04_Vars_Selection.html!\n",
    "\n",
    "\n",
    "### Al RFE y al Boruta PROBARLO CON UN HEAD DE 20.000 INSTANCIAS APROX! pq sino se nos va a trabar todo...\n",
    "\n",
    "# CLAVE: GRID SEARCH EN RANDOM FOREST! --> SACAR EL NRO DE ESTIMADORES OPTIMOS, NO poner n_estimators='auto' !\n",
    "\n",
    "# BUSCAR: XGBoost o RandomForest Feature Selection / Engeneering !\n",
    "\n",
    "## Trees:\n",
    "\n",
    "```Python\n",
    "# Dentro del algoritmo puedo ver las variables más importantes!! Esto en: XGBoost, GradientBoosting, RandomForest, Trees. Cualquier algoritmo de árboles!!!\n",
    "\n",
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.figure(figsize=(12, 20))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X_train_t.keys()[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577d4a9-49b6-418a-81bf-7707a38de0e8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Entonces:\n",
    "\n",
    "* En regresión, usar los métodos de Lasso y Ridge\n",
    "* En otros métodos, messirve el de los Trees!\n",
    "\n",
    "\n",
    "# IMPORTANTE: REVISAR CURVA ROC DEL TRAIN Y TMB DEL TEST!!!\n",
    "# Revisar cuanto cambia en AMBOS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaecfdb-2e25-4c8e-9011-6eacfcffd771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21788c8-d725-431c-aed6-6d3f8121dca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534934b4-e221-4241-915e-f738bfb9d1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf3f18-7257-4c20-ab12-31d3d2ef83ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
