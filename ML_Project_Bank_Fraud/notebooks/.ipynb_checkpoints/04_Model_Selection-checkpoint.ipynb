{
 "cells": [
  {
   "attachments": {
    "ed2b78cf-28e8-4b9f-b0a7-7a33715c6677.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "407f8ec0-ed64-4bd5-9d10-ca5482ce7b06",
   "metadata": {},
   "source": [
    "![image.png](attachment:ed2b78cf-28e8-4b9f-b0a7-7a33715c6677.png)\n",
    "\n",
    "# Máster en Data Science - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1513189-4152-4cf6-a597-f977ab246450",
   "metadata": {},
   "source": [
    "# Predicción de fraude mediante el uso de modelos de Machine Learning\n",
    "##### <font color='dodgerblue' face='Montserrat'>Autor: Rodrigo Fernandez Campos</font>\n",
    "## DataSet: Bank Account Fraud Dataset Suite (NeurIPS 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a09322-147f-44ca-b5ea-77e1e8add901",
   "metadata": {},
   "source": [
    "# <font size=25><b>Comparación y seleccion de Modelos</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dc0b4ffb-0b89-4773-847b-f3d5a886cb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from termcolor import colored, cprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score,\\\n",
    "                            accuracy_score,average_precision_score, precision_recall_curve, roc_curve,\\\n",
    "                            auc, recall_score, precision_score, confusion_matrix, f1_score\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf12c4-4755-441b-beba-a508611679aa",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3b7952-e55b-405d-a06a-cb3e39bbe766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../src/')\n",
    "import functions_rfc as fr\n",
    "sys.path.remove('../src/')\n",
    "\n",
    "### Constantes:\n",
    "seed=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26a6aa-3848-4a18-a679-e645ae18d797",
   "metadata": {},
   "source": [
    "***\n",
    "# Importo y preparo los datasets\n",
    "\n",
    "## Importo datasets procesados con anterioridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aec855-a0b9-4026-8329-a23c46e46517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_6h</th>\n",
       "      <th>velocity_24h</th>\n",
       "      <th>velocity_4w</th>\n",
       "      <th>bank_branch_count_8w</th>\n",
       "      <th>date_of_birth_distinct_emails_4w</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>credit_risk_score</th>\n",
       "      <th>email_is_free</th>\n",
       "      <th>housing_status</th>\n",
       "      <th>phone_home_valid</th>\n",
       "      <th>phone_mobile_valid</th>\n",
       "      <th>bank_months_count</th>\n",
       "      <th>has_other_cards</th>\n",
       "      <th>proposed_credit_limit</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>source</th>\n",
       "      <th>session_length_in_minutes</th>\n",
       "      <th>device_os_other</th>\n",
       "      <th>device_os_linux</th>\n",
       "      <th>device_os_windows</th>\n",
       "      <th>device_os_macintosh</th>\n",
       "      <th>device_os_x11</th>\n",
       "      <th>keep_alive_session</th>\n",
       "      <th>device_distinct_emails_8w</th>\n",
       "      <th>month</th>\n",
       "      <th>fraud_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.433984</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0.020608</td>\n",
       "      <td>49.172739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>954</td>\n",
       "      <td>5137.541683</td>\n",
       "      <td>2712.051960</td>\n",
       "      <td>4240.496920</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.396148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.712826</td>\n",
       "      <td>-1</td>\n",
       "      <td>97</td>\n",
       "      <td>50</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>-1.091855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1108</td>\n",
       "      <td>4370.169334</td>\n",
       "      <td>3097.535634</td>\n",
       "      <td>4383.873312</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.100053</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.222290</td>\n",
       "      <td>32</td>\n",
       "      <td>115</td>\n",
       "      <td>30</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>-1.067640</td>\n",
       "      <td>2.0</td>\n",
       "      <td>266</td>\n",
       "      <td>7168.731474</td>\n",
       "      <td>2865.219877</td>\n",
       "      <td>4207.649556</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.369306</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>-1</td>\n",
       "      <td>375</td>\n",
       "      <td>40</td>\n",
       "      <td>0.020157</td>\n",
       "      <td>-0.930885</td>\n",
       "      <td>2.0</td>\n",
       "      <td>962</td>\n",
       "      <td>4761.030105</td>\n",
       "      <td>6249.502233</td>\n",
       "      <td>6013.337906</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.700024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.506995</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>-0.698526</td>\n",
       "      <td>2.0</td>\n",
       "      <td>874</td>\n",
       "      <td>8823.184279</td>\n",
       "      <td>6315.937497</td>\n",
       "      <td>5653.839202</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.592456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income  name_email_similarity  prev_address_months_count  \\\n",
       "0     0.8               0.433984                         54   \n",
       "1     0.1               0.712826                         -1   \n",
       "2     0.4               0.222290                         32   \n",
       "3     0.7               0.476667                         -1   \n",
       "4     0.1               0.506995                         11   \n",
       "\n",
       "   current_address_months_count  customer_age  days_since_request  \\\n",
       "0                             9            30            0.020608   \n",
       "1                            97            50            0.024353   \n",
       "2                           115            30            0.002837   \n",
       "3                           375            40            0.020157   \n",
       "4                            10            20            0.007662   \n",
       "\n",
       "   intended_balcon_amount  payment_type  zip_count_4w  velocity_6h  \\\n",
       "0               49.172739           0.0           954  5137.541683   \n",
       "1               -1.091855           1.0          1108  4370.169334   \n",
       "2               -1.067640           2.0           266  7168.731474   \n",
       "3               -0.930885           2.0           962  4761.030105   \n",
       "4               -0.698526           2.0           874  8823.184279   \n",
       "\n",
       "   velocity_24h  velocity_4w  bank_branch_count_8w  \\\n",
       "0   2712.051960  4240.496920                     2   \n",
       "1   3097.535634  4383.873312                     0   \n",
       "2   2865.219877  4207.649556                     0   \n",
       "3   6249.502233  6013.337906                     0   \n",
       "4   6315.937497  5653.839202                     0   \n",
       "\n",
       "   date_of_birth_distinct_emails_4w  employment_status  credit_risk_score  \\\n",
       "0                                11                0.0                286   \n",
       "1                                 6                0.0                132   \n",
       "2                                11                0.0                171   \n",
       "3                                 8                0.0                245   \n",
       "4                                13                3.0                 20   \n",
       "\n",
       "   email_is_free  housing_status  phone_home_valid  phone_mobile_valid  \\\n",
       "0              0             0.0                 0                   1   \n",
       "1              0             1.0                 1                   1   \n",
       "2              0             4.0                 0                   1   \n",
       "3              1             0.0                 0                   1   \n",
       "4              0             2.0                 0                   1   \n",
       "\n",
       "   bank_months_count  has_other_cards  proposed_credit_limit  foreign_request  \\\n",
       "0                  1                1                 1000.0                0   \n",
       "1                 28                1                  200.0                0   \n",
       "2                 -1                1                  200.0                0   \n",
       "3                 -1                1                 1500.0                1   \n",
       "4                 -1                0                  200.0                0   \n",
       "\n",
       "   source  session_length_in_minutes  device_os_other  device_os_linux  \\\n",
       "0       0                  14.396148                1                0   \n",
       "1       0                   4.100053                1                0   \n",
       "2       0                   2.369306                1                0   \n",
       "3       0                  31.700024                0                1   \n",
       "4       0                   0.592456                0                1   \n",
       "\n",
       "   device_os_windows  device_os_macintosh  device_os_x11  keep_alive_session  \\\n",
       "0                  0                    0              0                   0   \n",
       "1                  0                    0              0                   0   \n",
       "2                  0                    0              0                   0   \n",
       "3                  0                    0              0                   1   \n",
       "4                  0                    0              0                   1   \n",
       "\n",
       "   device_distinct_emails_8w  month  fraud_bool  \n",
       "0                          1      6           0  \n",
       "1                          1      6           0  \n",
       "2                          1      5           0  \n",
       "3                          1      1           0  \n",
       "4                          1      2           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fraud_train = pd.read_csv('../data/processed/df_train_ready_to_model.csv')\n",
    "\n",
    "df_fraud_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab764a-f2a4-4da9-a117-38bdaabd07a7",
   "metadata": {},
   "source": [
    "Selecciono solo las variables elegidas en el último procesamiento del notebook anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52d72e9-687b-418c-ae72-da421d3df18c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fraud_train_cut = df_fraud_train.drop(['source','velocity_24h','velocity_6h','phone_mobile_valid', 'bank_months_count'], axis=1)\n",
    "\n",
    "df_fraud_train_cut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6c31c-d49f-4b89-9b69-0dc6cdef12b5",
   "metadata": {},
   "source": [
    "Separo en X e y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35679f6-6736-4276-b468-e2c7242ade49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_0 = df_fraud_train_cut.drop('fraud_bool', axis=1)\n",
    "y_train_0 = df_fraud_train_cut['fraud_bool']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a667516-9181-4f4f-8529-fdb6df6b7cca",
   "metadata": {},
   "source": [
    "## Separación en train y validación estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4786a54-9e2d-4d17-9be6-3bafdb950244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_1, X_val, y_train_1, y_val = train_test_split(df_fraud_train_cut.drop('fraud_bool',axis=1)\n",
    "                                                  ,df_fraud_train_cut['fraud_bool']\n",
    "                                                  ,stratify=df_fraud_train_cut['fraud_bool']\n",
    "                                                  ,test_size=0.3\n",
    "                                                  ,random_state=seed)\n",
    "\n",
    "X_train_complete, X_val_complete, y_train_complete, y_val_complete = train_test_split(df_fraud_train.drop('fraud_bool',axis=1)\n",
    "                                                                                      ,df_fraud_train['fraud_bool']\n",
    "                                                                                      ,stratify=df_fraud_train['fraud_bool']\n",
    "                                                                                      ,test_size=0.3\n",
    "                                                                                      ,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b58133-3199-42e8-8a29-7e444327fbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((490000, 29), (210000, 29), (490000, 34), (210000, 34))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape, X_val.shape, X_train_complete.shape, X_val_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b35fc9-aed9-4746-b519-b34674cb35ae",
   "metadata": {},
   "source": [
    "### Realizo el escalado para utilizar al aplicar el modelo de Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5dabb89-ab38-4241-bf30-0c73535c1bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_4w</th>\n",
       "      <th>bank_branch_count_8w</th>\n",
       "      <th>date_of_birth_distinct_emails_4w</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>credit_risk_score</th>\n",
       "      <th>email_is_free</th>\n",
       "      <th>housing_status</th>\n",
       "      <th>phone_home_valid</th>\n",
       "      <th>has_other_cards</th>\n",
       "      <th>proposed_credit_limit</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>session_length_in_minutes</th>\n",
       "      <th>device_os_other</th>\n",
       "      <th>device_os_linux</th>\n",
       "      <th>device_os_windows</th>\n",
       "      <th>device_os_macintosh</th>\n",
       "      <th>device_os_x11</th>\n",
       "      <th>keep_alive_session</th>\n",
       "      <th>device_distinct_emails_8w</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599278</th>\n",
       "      <td>-0.906378</td>\n",
       "      <td>-1.141368</td>\n",
       "      <td>-0.107470</td>\n",
       "      <td>-0.753237</td>\n",
       "      <td>-0.306349</td>\n",
       "      <td>-0.187361</td>\n",
       "      <td>-0.480608</td>\n",
       "      <td>-0.239490</td>\n",
       "      <td>-0.526812</td>\n",
       "      <td>1.239599</td>\n",
       "      <td>4.292476</td>\n",
       "      <td>0.496493</td>\n",
       "      <td>0.308335</td>\n",
       "      <td>0.259751</td>\n",
       "      <td>0.940482</td>\n",
       "      <td>0.180225</td>\n",
       "      <td>-0.846629</td>\n",
       "      <td>-0.535305</td>\n",
       "      <td>-0.030835</td>\n",
       "      <td>-0.161577</td>\n",
       "      <td>-0.337002</td>\n",
       "      <td>-0.721847</td>\n",
       "      <td>1.417951</td>\n",
       "      <td>-0.598906</td>\n",
       "      <td>-0.238795</td>\n",
       "      <td>-0.085935</td>\n",
       "      <td>-1.166552</td>\n",
       "      <td>-0.101765</td>\n",
       "      <td>-0.129402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499849</th>\n",
       "      <td>1.160536</td>\n",
       "      <td>1.227288</td>\n",
       "      <td>-0.402094</td>\n",
       "      <td>0.253942</td>\n",
       "      <td>0.524904</td>\n",
       "      <td>-0.188845</td>\n",
       "      <td>-0.506226</td>\n",
       "      <td>0.793958</td>\n",
       "      <td>1.384972</td>\n",
       "      <td>-0.595162</td>\n",
       "      <td>-0.400956</td>\n",
       "      <td>0.098949</td>\n",
       "      <td>-0.473747</td>\n",
       "      <td>0.302832</td>\n",
       "      <td>-1.063284</td>\n",
       "      <td>-0.607893</td>\n",
       "      <td>1.181155</td>\n",
       "      <td>1.868093</td>\n",
       "      <td>-0.646885</td>\n",
       "      <td>-0.161577</td>\n",
       "      <td>0.428755</td>\n",
       "      <td>-0.721847</td>\n",
       "      <td>-0.705243</td>\n",
       "      <td>-0.598906</td>\n",
       "      <td>4.187688</td>\n",
       "      <td>-0.085935</td>\n",
       "      <td>0.857227</td>\n",
       "      <td>-0.101765</td>\n",
       "      <td>0.775534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345337</th>\n",
       "      <td>-1.595349</td>\n",
       "      <td>1.292729</td>\n",
       "      <td>0.209817</td>\n",
       "      <td>-0.787187</td>\n",
       "      <td>-1.137602</td>\n",
       "      <td>-0.189010</td>\n",
       "      <td>-0.508535</td>\n",
       "      <td>-0.239490</td>\n",
       "      <td>0.148930</td>\n",
       "      <td>-0.242573</td>\n",
       "      <td>-0.396598</td>\n",
       "      <td>1.689126</td>\n",
       "      <td>-0.473747</td>\n",
       "      <td>0.460797</td>\n",
       "      <td>-1.063284</td>\n",
       "      <td>-0.607893</td>\n",
       "      <td>1.181155</td>\n",
       "      <td>-0.535305</td>\n",
       "      <td>-0.010300</td>\n",
       "      <td>-0.161577</td>\n",
       "      <td>-0.332120</td>\n",
       "      <td>1.385334</td>\n",
       "      <td>-0.705243</td>\n",
       "      <td>-0.598906</td>\n",
       "      <td>-0.238795</td>\n",
       "      <td>-0.085935</td>\n",
       "      <td>-1.166552</td>\n",
       "      <td>-0.101765</td>\n",
       "      <td>-0.129402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571314</th>\n",
       "      <td>-1.595349</td>\n",
       "      <td>-0.058074</td>\n",
       "      <td>-0.402094</td>\n",
       "      <td>-0.300572</td>\n",
       "      <td>0.524904</td>\n",
       "      <td>-0.189629</td>\n",
       "      <td>-0.503390</td>\n",
       "      <td>0.793958</td>\n",
       "      <td>-0.220290</td>\n",
       "      <td>0.473835</td>\n",
       "      <td>-0.400956</td>\n",
       "      <td>-0.298595</td>\n",
       "      <td>-0.473747</td>\n",
       "      <td>-0.788563</td>\n",
       "      <td>0.940482</td>\n",
       "      <td>-0.607893</td>\n",
       "      <td>-0.846629</td>\n",
       "      <td>-0.535305</td>\n",
       "      <td>-0.646885</td>\n",
       "      <td>-0.161577</td>\n",
       "      <td>2.296500</td>\n",
       "      <td>1.385334</td>\n",
       "      <td>-0.705243</td>\n",
       "      <td>-0.598906</td>\n",
       "      <td>-0.238795</td>\n",
       "      <td>-0.085935</td>\n",
       "      <td>-1.166552</td>\n",
       "      <td>-0.101765</td>\n",
       "      <td>-1.034339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379586</th>\n",
       "      <td>-0.561893</td>\n",
       "      <td>-0.499498</td>\n",
       "      <td>-0.130133</td>\n",
       "      <td>-0.877720</td>\n",
       "      <td>-0.306349</td>\n",
       "      <td>-0.186727</td>\n",
       "      <td>-0.476766</td>\n",
       "      <td>1.827405</td>\n",
       "      <td>1.479517</td>\n",
       "      <td>1.158247</td>\n",
       "      <td>-0.322514</td>\n",
       "      <td>1.291582</td>\n",
       "      <td>-0.473747</td>\n",
       "      <td>0.776728</td>\n",
       "      <td>0.940482</td>\n",
       "      <td>0.180225</td>\n",
       "      <td>-0.846629</td>\n",
       "      <td>-0.535305</td>\n",
       "      <td>2.022667</td>\n",
       "      <td>-0.161577</td>\n",
       "      <td>-0.130286</td>\n",
       "      <td>1.385334</td>\n",
       "      <td>-0.705243</td>\n",
       "      <td>-0.598906</td>\n",
       "      <td>-0.238795</td>\n",
       "      <td>-0.085935</td>\n",
       "      <td>-1.166552</td>\n",
       "      <td>-0.101765</td>\n",
       "      <td>-0.581871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          income  name_email_similarity  prev_address_months_count  \\\n",
       "599278 -0.906378              -1.141368                  -0.107470   \n",
       "499849  1.160536               1.227288                  -0.402094   \n",
       "345337 -1.595349               1.292729                   0.209817   \n",
       "571314 -1.595349              -0.058074                  -0.402094   \n",
       "379586 -0.561893              -0.499498                  -0.130133   \n",
       "\n",
       "        current_address_months_count  customer_age  days_since_request  \\\n",
       "599278                     -0.753237     -0.306349           -0.187361   \n",
       "499849                      0.253942      0.524904           -0.188845   \n",
       "345337                     -0.787187     -1.137602           -0.189010   \n",
       "571314                     -0.300572      0.524904           -0.189629   \n",
       "379586                     -0.877720     -0.306349           -0.186727   \n",
       "\n",
       "        intended_balcon_amount  payment_type  zip_count_4w  velocity_4w  \\\n",
       "599278               -0.480608     -0.239490     -0.526812     1.239599   \n",
       "499849               -0.506226      0.793958      1.384972    -0.595162   \n",
       "345337               -0.508535     -0.239490      0.148930    -0.242573   \n",
       "571314               -0.503390      0.793958     -0.220290     0.473835   \n",
       "379586               -0.476766      1.827405      1.479517     1.158247   \n",
       "\n",
       "        bank_branch_count_8w  date_of_birth_distinct_emails_4w  \\\n",
       "599278              4.292476                          0.496493   \n",
       "499849             -0.400956                          0.098949   \n",
       "345337             -0.396598                          1.689126   \n",
       "571314             -0.400956                         -0.298595   \n",
       "379586             -0.322514                          1.291582   \n",
       "\n",
       "        employment_status  credit_risk_score  email_is_free  housing_status  \\\n",
       "599278           0.308335           0.259751       0.940482        0.180225   \n",
       "499849          -0.473747           0.302832      -1.063284       -0.607893   \n",
       "345337          -0.473747           0.460797      -1.063284       -0.607893   \n",
       "571314          -0.473747          -0.788563       0.940482       -0.607893   \n",
       "379586          -0.473747           0.776728       0.940482        0.180225   \n",
       "\n",
       "        phone_home_valid  has_other_cards  proposed_credit_limit  \\\n",
       "599278         -0.846629        -0.535305              -0.030835   \n",
       "499849          1.181155         1.868093              -0.646885   \n",
       "345337          1.181155        -0.535305              -0.010300   \n",
       "571314         -0.846629        -0.535305              -0.646885   \n",
       "379586         -0.846629        -0.535305               2.022667   \n",
       "\n",
       "        foreign_request  session_length_in_minutes  device_os_other  \\\n",
       "599278        -0.161577                  -0.337002        -0.721847   \n",
       "499849        -0.161577                   0.428755        -0.721847   \n",
       "345337        -0.161577                  -0.332120         1.385334   \n",
       "571314        -0.161577                   2.296500         1.385334   \n",
       "379586        -0.161577                  -0.130286         1.385334   \n",
       "\n",
       "        device_os_linux  device_os_windows  device_os_macintosh  \\\n",
       "599278         1.417951          -0.598906            -0.238795   \n",
       "499849        -0.705243          -0.598906             4.187688   \n",
       "345337        -0.705243          -0.598906            -0.238795   \n",
       "571314        -0.705243          -0.598906            -0.238795   \n",
       "379586        -0.705243          -0.598906            -0.238795   \n",
       "\n",
       "        device_os_x11  keep_alive_session  device_distinct_emails_8w     month  \n",
       "599278      -0.085935           -1.166552                  -0.101765 -0.129402  \n",
       "499849      -0.085935            0.857227                  -0.101765  0.775534  \n",
       "345337      -0.085935           -1.166552                  -0.101765 -0.129402  \n",
       "571314      -0.085935           -1.166552                  -0.101765 -1.034339  \n",
       "379586      -0.085935           -1.166552                  -0.101765 -0.581871  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "model_scaled = scaler.fit(X_train_1)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_1), columns=X_train_1.columns, index=X_train_1.index)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "\n",
    "X_train_scaled.head() # muestro uno de ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebe22d-6d14-4631-a8d6-fba486d16a08",
   "metadata": {},
   "source": [
    "***\n",
    "# Unsersampling + Oversampling\n",
    "\n",
    "Investigando en internet me he topado con una técnica que me parece interesante, que es una combinación proporcional de undersampling y oversampling. He decidido probarlo mediante la aplicación de un Pipeline en donde se aplican secuencialmente las técnicas.\n",
    "\n",
    "Me parece una aplicación acertada para casos como el de este dataset en donde los datos están extremadamente desbalanceados, al punto de que un undersampling aplicado directamente eliminaría una cantidad demasiado alta de información, mientras que un oversampling podría generar un sesgo por la cantidad enorme de datos sintéticos a generar. Es por eso que un acercamiento entre ambos parece una solución interesante, al menos sobre el papel.\n",
    "\n",
    "Existen dentro de Imbalanced-Learning de SKL dos funciones con una combinación interna de oversampling y undersampling, pero al probarlas no parecen estar optimizadas y el costo computacional que me requieren es demasiado alto, por eso generaré y aplicaré mi propio Pipeline manualmente.\n",
    "\n",
    "Entonces, para lidiar con el desbalanceo terminaré aplicando:\n",
    "\n",
    "1. Split estatificado (SE) --> (realizado en Notebook 02)\n",
    "2. RandomUnderSampling\n",
    "3. SMOTE OverSampling\n",
    "4. Pipeline con sampling mix --> (SMOTE OverSampling + Random UnderSampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a93cfa-5293-4c40-8e91-9d9f107600bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((490000, 29),\n",
       " fraud_bool\n",
       " 0    484596\n",
       " 1      5404\n",
       " Name: count, dtype: int64,\n",
       " fraud_bool\n",
       " 0    0.988971\n",
       " 1    0.011029\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El tamaño y distribución actual del DataSet de train son los siguientes:\n",
    "X_train_1.shape,y_train_1.value_counts(),y_train_1.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558d820-39bf-478a-8e54-6f0a81f93ec8",
   "metadata": {},
   "source": [
    "#### 1. Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd206417-b76d-4d45-8d43-a3d8bb07dfd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "undersamp = RandomUnderSampler(sampling_strategy=0.08, random_state=seed)\n",
    "X_train_under, y_train_under = undersamp.fit_resample(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6728dd-8a52-41f4-8674-4a9da6884549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72954, 29),\n",
       " fraud_bool\n",
       " 0    67550\n",
       " 1     5404\n",
       " Name: count, dtype: int64,\n",
       " fraud_bool\n",
       " 0    0.925926\n",
       " 1    0.074074\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_under.shape,y_train_under.value_counts(),y_train_under.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7662016b-49dd-4b87-9b68-8d315a0c561f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8511142857142857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - X_train_under.shape[0] / X_train_1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6053bc-0d45-4832-a16b-fc7172705a38",
   "metadata": {},
   "source": [
    "Logramos el cometido, el dataset está balanceado, pero solo nos quedamos con 27k datos, es decir que perdimos el 94% de nuestro dataset, \n",
    "dato que es al menos recalcable. Habrá que ver como funcionan los modelos con una reducción tan grande del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0a8ca-b119-4639-8de3-dc624e4e5b8c",
   "metadata": {},
   "source": [
    "#### 2. SMOTE OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c24333fc-0302-4a6c-b7d3-6efda57c89e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oversamp = SMOTE(sampling_strategy=0.25, random_state=seed)\n",
    "X_train_over, y_train_over = oversamp.fit_resample(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "214cb8ed-bd51-481b-b4a3-21acb5f1382c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((605745, 29),\n",
       " fraud_bool\n",
       " 0    484596\n",
       " 1    121149\n",
       " Name: count, dtype: int64,\n",
       " fraud_bool\n",
       " 0    0.8\n",
       " 1    0.2\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_over.shape,y_train_over.value_counts(),y_train_over.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d33051e-df02-4d8e-838f-2aaef3e059e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115745"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_over.value_counts()[1] - y_train_1.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7d3f9e-7047-4125-90a8-5ea272bc984c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2362142857142857"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train_over.value_counts()[1] - y_train_1.value_counts()[1]) / y_train_1.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d410f90b-1342-4975-8938-c0fcc7b1636d",
   "metadata": {},
   "source": [
    "Con este método generamos 115k nuevos registros, es decir que agrandamos nuestro dataset de manera sintética casi un 24%, lo cual parece bastante,\n",
    "pero habrá que comprobarlo con los resultados del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856ab3f-30c4-4773-85e9-cf80d686205d",
   "metadata": {},
   "source": [
    "#### 3. Mix con Pipeline (SMOTE OverSampling + UnderSampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49778472-e63e-448d-9210-b72c011dfc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.2, random_state=seed)\n",
    "under = RandomUnderSampler(sampling_strategy=0.25, random_state=seed) # Voy a buscar una proporción de aprox 60%-40%\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X_train_mix, y_train_mix = pipeline.fit_resample(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc92b167-6f19-479b-a46d-f4649bbd359a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((484595, 29),\n",
       " fraud_bool\n",
       " 0    387676\n",
       " 1     96919\n",
       " Name: count, dtype: int64,\n",
       " fraud_bool\n",
       " 0    0.8\n",
       " 1    0.2\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mix.shape,y_train_mix.value_counts(),y_train_mix.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e977f4-81e0-4bf8-8234-750dc20a3467",
   "metadata": {
    "tags": []
   },
   "source": [
    "Vemos que de esta manera reducimos a poco más de la mitad los datos originales de train y la cantidad de datos a generar sintéticamente\n",
    "\n",
    "El tamaño del dataset no es minúsculo ni se expandió de manera tan abrupta, sino que casi que se mantuvo en su tamaño gracias al mix aplicado.\n",
    "\n",
    "Por supuesto que nunca vamos a llegar a una solución perfecta, pero esta pareciese ser la más razonable por el momento, aunque sin haberla medido. La hora de la verdad llegará cuando comencemos a probar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00381f3a-9341-45a7-a54b-706423323ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_4w</th>\n",
       "      <th>bank_branch_count_8w</th>\n",
       "      <th>date_of_birth_distinct_emails_4w</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>credit_risk_score</th>\n",
       "      <th>email_is_free</th>\n",
       "      <th>housing_status</th>\n",
       "      <th>phone_home_valid</th>\n",
       "      <th>has_other_cards</th>\n",
       "      <th>proposed_credit_limit</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>session_length_in_minutes</th>\n",
       "      <th>device_os_other</th>\n",
       "      <th>device_os_linux</th>\n",
       "      <th>device_os_windows</th>\n",
       "      <th>device_os_macintosh</th>\n",
       "      <th>device_os_x11</th>\n",
       "      <th>keep_alive_session</th>\n",
       "      <th>device_distinct_emails_8w</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339345</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.129004</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>-0.760933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2034</td>\n",
       "      <td>4847.050264</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.150540</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453683</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.231047</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>-1.319822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1076</td>\n",
       "      <td>4801.842797</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.680771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198425</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.550407</td>\n",
       "      <td>-1</td>\n",
       "      <td>324</td>\n",
       "      <td>50</td>\n",
       "      <td>0.028341</td>\n",
       "      <td>-0.604352</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1228</td>\n",
       "      <td>5519.625841</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.389204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165400</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.682156</td>\n",
       "      <td>-1</td>\n",
       "      <td>252</td>\n",
       "      <td>40</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>-0.983700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1239</td>\n",
       "      <td>4377.519574</td>\n",
       "      <td>2119</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.341758</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72103</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.278175</td>\n",
       "      <td>98</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>0.517795</td>\n",
       "      <td>-0.709431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1049</td>\n",
       "      <td>4312.921908</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.257548</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        income  name_email_similarity  prev_address_months_count  \\\n",
       "339345     0.8               0.129004                         56   \n",
       "453683     0.1               0.231047                         31   \n",
       "198425     0.7               0.550407                         -1   \n",
       "165400     0.4               0.682156                         -1   \n",
       "72103      0.9               0.278175                         98   \n",
       "\n",
       "        current_address_months_count  customer_age  days_since_request  \\\n",
       "339345                            10            30            0.016274   \n",
       "453683                             3            20            0.007882   \n",
       "198425                           324            50            0.028341   \n",
       "165400                           252            40            0.019537   \n",
       "72103                             20            40            0.517795   \n",
       "\n",
       "        intended_balcon_amount  payment_type  zip_count_4w  velocity_4w  \\\n",
       "339345               -0.760933           1.0          2034  4847.050264   \n",
       "453683               -1.319822           1.0          1076  4801.842797   \n",
       "198425               -0.604352           3.0          1228  5519.625841   \n",
       "165400               -0.983700           1.0          1239  4377.519574   \n",
       "72103                -0.709431           1.0          1049  4312.921908   \n",
       "\n",
       "        bank_branch_count_8w  date_of_birth_distinct_emails_4w  \\\n",
       "339345                    10                                10   \n",
       "453683                    12                                11   \n",
       "198425                   174                                 2   \n",
       "165400                  2119                                 2   \n",
       "72103                     35                                11   \n",
       "\n",
       "        employment_status  credit_risk_score  email_is_free  housing_status  \\\n",
       "339345                0.0                109              1             2.0   \n",
       "453683                0.0                 96              1             2.0   \n",
       "198425                1.0                218              0             0.0   \n",
       "165400                0.0                114              0             4.0   \n",
       "72103                 0.0                 53              0             0.0   \n",
       "\n",
       "        phone_home_valid  has_other_cards  proposed_credit_limit  \\\n",
       "339345                 0                1                  200.0   \n",
       "453683                 0                0                  500.0   \n",
       "198425                 0                0                 1500.0   \n",
       "165400                 1                0                  200.0   \n",
       "72103                  0                0                  500.0   \n",
       "\n",
       "        foreign_request  session_length_in_minutes  device_os_other  \\\n",
       "339345                0                   5.150540                0   \n",
       "453683                0                  15.680771                0   \n",
       "198425                0                   8.389204                0   \n",
       "165400                0                   3.341758                0   \n",
       "72103                 0                   3.257548                0   \n",
       "\n",
       "        device_os_linux  device_os_windows  device_os_macintosh  \\\n",
       "339345                1                  0                    0   \n",
       "453683                0                  0                    1   \n",
       "198425                0                  1                    0   \n",
       "165400                0                  0                    1   \n",
       "72103                 0                  0                    1   \n",
       "\n",
       "        device_os_x11  keep_alive_session  device_distinct_emails_8w  month  \n",
       "339345              0                   1                          1      3  \n",
       "453683              0                   1                          1      4  \n",
       "198425              0                   0                          1      2  \n",
       "165400              0                   1                          1      5  \n",
       "72103               0                   0                          1      5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3e4421-b97c-40d8-b71d-eaad81db4d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restauro el index:\n",
    "X_train_mix = X_train_mix.reset_index(drop=True)\n",
    "y_train_mix = y_train_mix.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2c457-2d23-4851-b4ec-f53baec582db",
   "metadata": {},
   "source": [
    "## Añadido de PCA como prueba para mejora del modelo\n",
    "\n",
    "Además de los tratamientos realizados, generaré otro dataframe igual a X_train pero con las dos variables de PCA con el fin de, como se comentó en el notebook anterior, probar si mejoran el modelo o si por el contrario solo agregan ruido.\n",
    "\n",
    "Para ello importaré el modelo ya entrenado en el notebook anterior. Deberé aplicarlo primero sobre el Dataset con todas las variables y luego recortarlo (al haberse usado la misma semilla son exactamente iguales el completo y el recortado con la única diferencia de que uno tiene más variables que el otro):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9928010-45a6-4b05-a422-08527a84b94c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = load('../models/pca_2components.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f33ed99-dd51-402b-b299-844653b73c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>velocity_4w</th>\n",
       "      <th>bank_branch_count_8w</th>\n",
       "      <th>date_of_birth_distinct_emails_4w</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>credit_risk_score</th>\n",
       "      <th>email_is_free</th>\n",
       "      <th>housing_status</th>\n",
       "      <th>phone_home_valid</th>\n",
       "      <th>has_other_cards</th>\n",
       "      <th>proposed_credit_limit</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>session_length_in_minutes</th>\n",
       "      <th>device_os_other</th>\n",
       "      <th>device_os_linux</th>\n",
       "      <th>device_os_windows</th>\n",
       "      <th>device_os_macintosh</th>\n",
       "      <th>device_os_x11</th>\n",
       "      <th>keep_alive_session</th>\n",
       "      <th>device_distinct_emails_8w</th>\n",
       "      <th>month</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599278</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.164202</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>0.014626</td>\n",
       "      <td>-1.047699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1043</td>\n",
       "      <td>5997.273703</td>\n",
       "      <td>2154</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.839120</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6563.105547</td>\n",
       "      <td>3457.992940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499849</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.848983</td>\n",
       "      <td>-1</td>\n",
       "      <td>109</td>\n",
       "      <td>40</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>-1.566874</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2964</td>\n",
       "      <td>4310.932705</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.010277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5628.070298</td>\n",
       "      <td>2650.632267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345337</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.867902</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005707</td>\n",
       "      <td>-1.613666</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1722</td>\n",
       "      <td>4635.000046</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.878469</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6832.737467</td>\n",
       "      <td>3278.452468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571314</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.477383</td>\n",
       "      <td>-1</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>-1.509390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1351</td>\n",
       "      <td>5293.455388</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.062248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9203.977628</td>\n",
       "      <td>4184.075173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379586</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.349767</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>-0.969820</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3059</td>\n",
       "      <td>5922.503266</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.505027</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7198.967957</td>\n",
       "      <td>3899.272516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        income  name_email_similarity  prev_address_months_count  \\\n",
       "599278     0.3               0.164202                         12   \n",
       "499849     0.9               0.848983                         -1   \n",
       "345337     0.1               0.867902                         26   \n",
       "571314     0.1               0.477383                         -1   \n",
       "379586     0.4               0.349767                         11   \n",
       "\n",
       "        current_address_months_count  customer_age  days_since_request  \\\n",
       "599278                            20            30            0.014626   \n",
       "499849                           109            40            0.006597   \n",
       "345337                            17            20            0.005707   \n",
       "571314                            60            40            0.002358   \n",
       "379586                             9            30            0.018052   \n",
       "\n",
       "        intended_balcon_amount  payment_type  zip_count_4w  velocity_4w  \\\n",
       "599278               -1.047699           1.0          1043  5997.273703   \n",
       "499849               -1.566874           2.0          2964  4310.932705   \n",
       "345337               -1.613666           1.0          1722  4635.000046   \n",
       "571314               -1.509390           2.0          1351  5293.455388   \n",
       "379586               -0.969820           3.0          3059  5922.503266   \n",
       "\n",
       "        bank_branch_count_8w  date_of_birth_distinct_emails_4w  \\\n",
       "599278                  2154                                12   \n",
       "499849                     0                                10   \n",
       "345337                     2                                18   \n",
       "571314                     0                                 8   \n",
       "379586                    36                                16   \n",
       "\n",
       "        employment_status  credit_risk_score  email_is_free  housing_status  \\\n",
       "599278                1.0                149              1             2.0   \n",
       "499849                0.0                152              0             1.0   \n",
       "345337                0.0                163              0             1.0   \n",
       "571314                0.0                 76              1             1.0   \n",
       "379586                0.0                185              1             2.0   \n",
       "\n",
       "        phone_home_valid  has_other_cards  proposed_credit_limit  \\\n",
       "599278                 0                0                  500.0   \n",
       "499849                 1                1                  200.0   \n",
       "345337                 1                0                  510.0   \n",
       "571314                 0                0                  200.0   \n",
       "379586                 0                0                 1500.0   \n",
       "\n",
       "        foreign_request  session_length_in_minutes  device_os_other  \\\n",
       "599278                0                   4.839120                0   \n",
       "499849                0                  11.010277                0   \n",
       "345337                0                   4.878469                1   \n",
       "571314                0                  26.062248                1   \n",
       "379586                0                   6.505027                1   \n",
       "\n",
       "        device_os_linux  device_os_windows  device_os_macintosh  \\\n",
       "599278                1                  0                    0   \n",
       "499849                0                  0                    1   \n",
       "345337                0                  0                    0   \n",
       "571314                0                  0                    0   \n",
       "379586                0                  0                    0   \n",
       "\n",
       "        device_os_x11  keep_alive_session  device_distinct_emails_8w  month  \\\n",
       "599278              0                   0                          1      3   \n",
       "499849              0                   1                          1      5   \n",
       "345337              0                   0                          1      3   \n",
       "571314              0                   0                          1      1   \n",
       "379586              0                   0                          1      2   \n",
       "\n",
       "               PCA1         PCA2  \n",
       "599278  6563.105547  3457.992940  \n",
       "499849  5628.070298  2650.632267  \n",
       "345337  6832.737467  3278.452468  \n",
       "571314  9203.977628  4184.075173  \n",
       "379586  7198.967957  3899.272516  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca = X_train_1.copy()\n",
    "X_val_pca = X_val.copy()\n",
    "X_train_pca['PCA1'], X_train_pca['PCA2'] = pca.transform(X_train_complete)[:,0], pca.transform(X_train_complete)[:,1]\n",
    "X_val_pca['PCA1'], X_val_pca['PCA2'] = pca.transform(X_val_complete)[:,0], pca.transform(X_val_complete)[:,1]\n",
    "X_train_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c51eab-b547-48ff-964c-1d05fc1850a8",
   "metadata": {},
   "source": [
    "## Resumen del tratamiento previo al modelado\n",
    "\n",
    "Una vez que he importado todos los datasets, generado sets de validación para comparar los modelos y agregar procesos de oversampling y undersampling, voy a proceder a la parte del modelado.\n",
    "\n",
    "El proceso de comentará a continuación en el siguiente apartado, pero antes de continuar voy a listar los datasets generados con los que voy a trabajar:\n",
    "\n",
    "1. Set sin recortes\n",
    "2. Set procesado con variables seleccionadas (5 menos que el original importado gracias al Feature Selection)\n",
    "3. Set escalado\n",
    "4. Set con variables PCA\n",
    "5. Set con Undersampling\n",
    "6. Set con Oversampling\n",
    "7. Set con mix Over y Under sampling.\n",
    "\n",
    "Aunque parecen muchos, son bastante similares entre si y la idea es realizar una validación numérica de con cual de ellos se obtendrían a priori mejores resultados, para no basarnos simplemente en intuición a la hora de buscar la optimización del uso de los datos disponibles en el modelo que elijamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9049d-8145-483c-a41d-32ba4c3ece06",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Modelado\n",
    "\n",
    "En este apartado de trataran los distintos modelos elegidos con los datasets disponibles. La idea es llegar al final del notebook con la elección tanto del modelo a utilizar, del dataset que mejor se adapte al mismo y de los hiperparámetros que optimizen los resultados buscados. Para lograrlo realizaré 3 pasos:\n",
    "\n",
    "1. Comparativa de modelos: Mediante un pipeline se probarán todos los modelos sin ajustar hiperparámetros, solo con sus valores por defecto, para ver cual es el que mejores resultados base proporciona.\n",
    "\n",
    "2. Comparativa de datasets: Con el modelo elegido se procederá a medir cual es el set de datos que mejores métricas obtiene.\n",
    "\n",
    "3. Búsqueda de hiperparámetros: Finalmente se realizará una búsqueda de los hiperparámetros que optimizen al modelo elegido en el dataset elegido mediante una Validación Cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2de22-2c2b-4471-97f5-486b658776e9",
   "metadata": {},
   "source": [
    "## Modelo Base:\n",
    "\n",
    "En primer lugar voy a realizar un modelo sin machine learning, en donde simplemente se asignará el valor de la clase mayoritaria a todos los resultados. En este caso, consideraremos que ninguna aplicación será fraudulenta. Esto es lo que haríamos si no pudiesemos aplicar ningún modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d9ba3c0-3295-41db-9d70-2fb5e7a146af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def y_pred_base_model(y_train_1, X_val):\n",
    "    value_max = y_train_1.value_counts(normalize=True).argmax()\n",
    "    size = X_val.index.value_counts().size\n",
    "    y_pred_base = np.random.choice([value_max, abs(1-value_max)], size=size, p=[abs(1-value_max),value_max])\n",
    "                  # preparé la función para dar valores random en otros casos, pero en este caso los valores serán una probabilidad fija.\n",
    "    return y_pred_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7203565-5b57-4d38-9144-a3b6862b390a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_base = y_pred_base_model(y_train_1,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4640115-aef0-458e-9b55-0aa84309c79c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], dtype=int64), (210000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_base, y_pred_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88bd6266-5e17-49f4-828a-d3fd528c5639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9889714285714286, 0.0, 0.0, 0.0, 0.5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val,y_pred_base), f1_score(y_val,y_pred_base), precision_score(y_val,y_pred_base), recall_score(y_val,y_pred_base), balanced_accuracy_score(y_val,y_pred_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbab31fa-8d91-491d-a371-1634d2467091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[207684,      0],\n",
       "       [  2316,      0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_val,y_pred_base)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59d1670d-0764-4c5f-948c-cb3ed79d3d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98897143, 0.        ],\n",
       "       [0.01102857, 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix_perc = cf_matrix/np.sum(cf_matrix)\n",
    "cf_matrix_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b4691f9-042b-412d-9503-d8a7c350d905",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = sns.heatmap(cf_matrix_perc, annot=True, fmt='.2%', cmap='Blues', linewidth=2)\n",
    "#plt.xlabel('Predictions', fontdict={'fontname':'Montserrat'})\n",
    "#plt.ylabel('Reality', fontdict={'fontname':'Montserrat'})\n",
    "plt.title('Confusion Matrix - Base Model', fontdict={'fontname':'Montserrat', 'fontsize':18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c9729-ef94-4f4b-a363-3760addc909c",
   "metadata": {},
   "source": [
    "Una vez contamos con el modelo base, procedemos a realizar la comparación de modelos con Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a3876-bb6d-4f3f-9899-1f8c622b82a4",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Comparación de Modelos\n",
    "\n",
    "A continuación ejecutaré un pipeline con algunos de los modelos de clasificación \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43976e19-2a56-47a3-8e4d-280afa392404",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Antes de realizar el pipeline, voy a ejecutar la regresión logística, que a diferencia de los demás, necesita utilizar el dataset escalado.\n",
    "\n",
    "Este es un modelo de tipo lineal (GLM) que permite obtener resultados adecuados para problemas de clasificación binara, por lo que se adapta muy bien a nuestro objetivo a priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c876474c-6279-4ae0-a060-35658f503b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy = 0.9889619047619047\n",
      "precision = 0.4444444444444444\n",
      "recall = 0.0034542314335060447\n",
      "f1_score = 0.006855184233076264\n",
      "confusion matrix:\n",
      "[[207674     10]\n",
      " [  2308      8]]\n",
      "\n",
      "CPU times: total: 2.94 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(C=1)\n",
    "lr.fit(X_train_scaled, y_train_1)\n",
    "\n",
    "accuracy = accuracy_score(y_val,lr.predict(X_val_scaled))\n",
    "balanced_acc = balanced_accuracy_score(y_val,lr.predict(X_val_scaled))\n",
    "f1 = f1_score(y_val,lr.predict(X_val_scaled)) # (y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_val,lr.predict(X_val_scaled))\n",
    "precision = precision_score(y_val,lr.predict(X_val_scaled))\n",
    "recall = recall_score(y_val,lr.predict(X_val_scaled))\n",
    "print(f\"\"\"\n",
    "accuracy = {accuracy}\n",
    "balanced accuracy = {balanced_acc}\n",
    "precision = {precision}\n",
    "recall = {recall}\n",
    "f1_score = {f1}\n",
    "confusion matrix:\n",
    "{c_matrix}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f314952-8a30-4043-b9f2-b00198da988b",
   "metadata": {},
   "source": [
    "Los resultados han sido un poco decepcionantes, debido a que es levemente superior al modelo base. Son muy pocos los registros que consigue determinar como\n",
    "fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03b6a5-a97b-4b78-b961-0ec7319cdf30",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "Este modelo es conocido por ser especialmente demandante computacionalmente hablando, por lo que en este caso procederé a probarlo con el dataset más pequeño posible. Debido a su alto coste computacional y a que no tiene un rendimiento tan bueno a priori como otros modelos que probaremos a continuación, considero adecuado probarlo con un dataset menor, ya que no tengo altas expectativas en cuanto a su desempeño.\n",
    "\n",
    "En vez de reducir los datos a un porcentaje, voy a aprovechar que ya poseo un dataset más pequeño, que en este caso es el que ha pasado por el proceso de Undersampling.\n",
    "\n",
    "###### *Nota: Es posible que este modelo obtenga mejores métricas debido a que está trabajando con un dataset balanceado. De igual manera, será comparado vs el mejor modelo elegido comprobando sus mejores métricas en varios DF de prueba. Por lo que, en caso de obtener buenos resultados, deberán compararse vs el otro modelo elegido en el mismo dataset con undersampling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b871ce8c-e53a-4fe0-b2ce-e4939d12d257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy = 0.9889714285714286\n",
      "balanced accuracy = 0.5\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "f1_score = 0.0\n",
      "confusion matrix:\n",
      "[[207684      0]\n",
      " [  2316      0]]\n",
      "\n",
      "CPU times: total: 33min 17s\n",
      "Wall time: 38min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = SVC(kernel=\"rbf\", degree=3) # utilizo los valores por defecto. Solo los dejo aclarados para mejor entendimiento.\n",
    "svc.fit(X_train_under, y_train_under)\n",
    "\n",
    "accuracy = accuracy_score(y_val,svc.predict(X_val))\n",
    "balanced_acc = balanced_accuracy_score(y_val,svc.predict(X_val))\n",
    "f1 = f1_score(y_val,svc.predict(X_val)) # (y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_val,svc.predict(X_val))\n",
    "precision = precision_score(y_val,svc.predict(X_val))\n",
    "recall = recall_score(y_val,svc.predict(X_val))\n",
    "print(f\"\"\"\n",
    "accuracy = {accuracy}\n",
    "balanced accuracy = {balanced_acc}\n",
    "precision = {precision}\n",
    "recall = {recall}\n",
    "f1_score = {f1}\n",
    "confusion matrix:\n",
    "{c_matrix}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab1f43-7eda-4d50-87f9-af6eb98ce131",
   "metadata": {},
   "source": [
    "La aplicación en este caso fue un fracaso. Además de la enorme cantidad de tiempo que demoró en comparación con otros modelos, la predicción simplemente\n",
    "actuó como si se tratara del modelo base, por lo que el SVC queda descartado de los posibles modelos a elegir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2db4ec-c7d2-4af0-8f12-a7f1d25f2128",
   "metadata": {},
   "source": [
    "## Pipeline con los demás modelos\n",
    "\n",
    "Ahora si procedo a probar el resto de los modelos en un pipeline:\n",
    "\n",
    "* DummyClassifier: Incluyo lo que sería un modelo base de SKLearn\n",
    "* GaussianNB: El clasificador de Naive Bayes, basado en las teorías de probabilidad Gaussianas, un tipo de modelo bastante eficiente en aprendizaje supervisado.\n",
    "* Decision Tree Classifier: Árbol de decisión simple\n",
    "* Random Forest Classifier: Emsamblaje de muchos clasificadores de árbol de decisión, en representación de los modelos de Bagging\n",
    "* XGBoost Classifier: Representante de los modelos de boosting, uno de los más efectivos actualmente en ML.\n",
    "* LightGBM Classifier: También representante de los modelos de boosting, de mis favoritos actualmente debido a su eficiencia y sobre todo a su gran optimización en cuanto a los tiempos de procesamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97c155a4-6896-4e38-8b98-b7ee2ac2fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDummyClassifier()\u001b[0m\n",
      "model score: 0.989\n",
      "\n",
      "    accuracy = 0.9889714285714286\n",
      "    balanced accuracy = 0.5\n",
      "    precision = 0.0\n",
      "    recall = 0.0\n",
      "    \u001b[1mf1_score = 0.0\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207684      0]\n",
      " [  2316      0]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "\u001b[1mGaussianNB()\u001b[0m\n",
      "model score: 0.928\n",
      "\n",
      "    accuracy = 0.9282714285714285\n",
      "    balanced accuracy = 0.6573891577808574\n",
      "    precision = 0.0607209318354125\n",
      "    recall = 0.3803972366148532\n",
      "    \u001b[1mf1_score = 0.10472511144130758\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[194056  13628]\n",
      " [  1435    881]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "\u001b[1mDecisionTreeClassifier()\u001b[0m\n",
      "model score: 0.977\n",
      "\n",
      "    accuracy = 0.9770619047619048\n",
      "    balanced accuracy = 0.5400909367789027\n",
      "    precision = 0.07364473235594954\n",
      "    recall = 0.09326424870466321\n",
      "    \u001b[1mf1_score = 0.08230139074109354\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[204967   2717]\n",
      " [  2100    216]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "\u001b[1mRandomForestClassifier()\u001b[0m\n",
      "model score: 0.989\n",
      "\n",
      "    accuracy = 0.9889761904761905\n",
      "    balanced accuracy = 0.5010698173081404\n",
      "    precision = 0.5555555555555556\n",
      "    recall = 0.002158894645941278\n",
      "    \u001b[1mf1_score = 0.004301075268817204\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207680      4]\n",
      " [  2311      5]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "\u001b[1mXGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\u001b[0m\n",
      "model score: 0.989\n",
      "\n",
      "    accuracy = 0.9888190476190476\n",
      "    balanced accuracy = 0.5195633002829229\n",
      "    precision = 0.42592592592592593\n",
      "    recall = 0.039723661485319514\n",
      "    \u001b[1mf1_score = 0.07266982622432859\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207560    124]\n",
      " [  2224     92]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "[LightGBM] [Info] Number of positive: 5404, number of negative: 484596\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2672\n",
      "[LightGBM] [Info] Number of data points in the train set: 490000, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496176\n",
      "[LightGBM] [Info] Start training from score -4.496176\n",
      "\u001b[1mLGBMClassifier()\u001b[0m\n",
      "model score: 0.989\n",
      "\n",
      "    accuracy = 0.9885761904761905\n",
      "    balanced accuracy = 0.5232831928897959\n",
      "    precision = 0.36303630363036304\n",
      "    recall = 0.047495682210708115\n",
      "    \u001b[1mf1_score = 0.08400152730049637\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207491    193]\n",
      " [  2206    110]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "CPU times: total: 8min 10s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifiers = [\n",
    "    DummyClassifier() # incluyo lo que sería un modelo base de SKLearn\n",
    "    ,GaussianNB() # Clasificador de NaiveBayes\n",
    "    ,DecisionTreeClassifier() # Árbol de decisión simple\n",
    "    ,RandomForestClassifier() # Random Forest en representación de modelos de Bagging\n",
    "    ,XGBClassifier() # Modelo boosting\n",
    "    ,LGBMClassifier() # Modelo boosting\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('classifier', classifier)])\n",
    "    pipe.fit(X_train_1, y_train_1)   \n",
    "    accuracy = accuracy_score(y_val,pipe.predict(X_val))\n",
    "    balanced_acc = balanced_accuracy_score(y_val,pipe.predict(X_val))\n",
    "    f1 = f1_score(y_val,pipe.predict(X_val)) # (y_true, y_pred)\n",
    "    c_matrix = confusion_matrix(y_val,pipe.predict(X_val))\n",
    "    precision = precision_score(y_val,pipe.predict(X_val))\n",
    "    recall = recall_score(y_val,pipe.predict(X_val))\n",
    "    print(f'\\033[1m{classifier}\\033[0m')\n",
    "    print(\"model score: %.3f\" % pipe.score(X_val, y_val))\n",
    "    print(f\"\"\"\n",
    "    accuracy = {accuracy}\n",
    "    balanced accuracy = {balanced_acc}\n",
    "    precision = {precision}\n",
    "    recall = {recall}\n",
    "    \\033[1mf1_score = {f1}\\033[0m\n",
    "    confusion matrix:\n",
    "    {c_matrix}\n",
    "    ____________________________________________________________________________________________\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922d14f-0d8f-4999-b30d-37189fc455e0",
   "metadata": {},
   "source": [
    "## Modelo a seleccionar\n",
    "\n",
    "Una vez ejecutados los modelos de clasificación, procederé a realizar la elección final del modelo con el que trabajaré.\n",
    "\n",
    "De entrada quedan descartados modelos como el Dummy por obvias razones y los modelos de Regresión Logística y SVM, por motivos mencionados anteriormente. Además, se descartan el modelo simple de Decision Tree y el Random Forest, debido a que sus métricas y rendimiento no son tan buenas como otros modelos a mencionar. Sobre todo el Random Forest ha decepcionado en este caso, aunque puede deberse a que no tiene una hiperparametrización correcta.\n",
    "\n",
    "Entre otros posibles elegidos entran el GaussianNB y el XGBoost. El primero con un bastante buen F1, aunque cediendo bastante Accuracy. El segundo, con un rendimiento menor pero aún así destacable por el hecho de que no posee ningún tipo de parametrización.\n",
    "\n",
    "Una desventaja de el GaussianNB es que no se puede hacer una búsqueda exaustiva de mejora de hiperparámetros como por ejemplo si podría hacer con los modelos de bagging o boosting, por ello he decidido priorizar modelos como el LightGBM en este caso.\n",
    "\n",
    "El modelo LightGBM, además de haber obtenido buenas métricas (el mejor F1 por detrás del GaussianNB pero sin), es el mejor en cuanto a tiempos de procesamiento de entre los modelos más complejos elegidos (bagging y boosting) y a su vez posee una capacidad de mejora seguramente superior al GaussianNB, por lo que lo considero el mejor modelo de entre los probados y con el que procederé a trabajar en los siguientes pasos del proceso. De igual manera, una vez hiperparametrizado, si no mejora al GaussianNB, podría replantearme esta decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23412e8f-30aa-4063-a22b-a63b7598e67f",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Comparación de datasets\n",
    "\n",
    "Con el modelo elegido, en este caso el LightGBM, voy a comprobar que set de datos obtiene mejores métricas.\n",
    "\n",
    "Como ya conozco las del dataset base (train_1), voy a probar con las demás variantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07dbb42f-a9d8-477a-ba19-9fc5c4e12414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5404, number of negative: 484596\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3218\n",
      "[LightGBM] [Info] Number of data points in the train set: 490000, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496176\n",
      "[LightGBM] [Info] Start training from score -4.496176\n",
      "\u001b[1mSet completo\u001b[0m\n",
      "model score: 0.989\n",
      "\n",
      "    accuracy = 0.9885571428571429\n",
      "    balanced accuracy = 0.523060080914079\n",
      "    precision = 0.35737704918032787\n",
      "    recall = 0.04706390328151986\n",
      "    \u001b[1mf1_score = 0.08317436093094238\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207488    196]\n",
      " [  2207    109]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "[LightGBM] [Info] Number of positive: 5404, number of negative: 484596\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3182\n",
      "[LightGBM] [Info] Number of data points in the train set: 490000, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496176\n",
      "[LightGBM] [Info] Start training from score -4.496176\n",
      "\u001b[1mSet con PCA\u001b[0m\n",
      "model score: 0.989\n",
      "\n",
      "    accuracy = 0.9885380952380952\n",
      "    balanced accuracy = 0.5217695591339293\n",
      "    precision = 0.3468013468013468\n",
      "    recall = 0.04447322970639033\n",
      "    \u001b[1mf1_score = 0.07883658629927287\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207490    194]\n",
      " [  2213    103]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "[LightGBM] [Info] Number of positive: 5404, number of negative: 67550\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2671\n",
      "[LightGBM] [Info] Number of data points in the train set: 72954, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074074 -> initscore=-2.525729\n",
      "[LightGBM] [Info] Start training from score -2.525729\n",
      "\u001b[1mSet Undersampling\u001b[0m\n",
      "model score: 0.980\n",
      "\n",
      "    accuracy = 0.9795809523809523\n",
      "    balanced accuracy = 0.6310269298125599\n",
      "    precision = 0.1960542540073983\n",
      "    recall = 0.27461139896373055\n",
      "    \u001b[1mf1_score = 0.22877697841726619\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[205076   2608]\n",
      " [  1680    636]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "[LightGBM] [Info] Number of positive: 121149, number of negative: 484596\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3968\n",
      "[LightGBM] [Info] Number of data points in the train set: 605745, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.200000 -> initscore=-1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "\u001b[1mSet Oversampling\u001b[0m\n",
      "model score: 0.988\n",
      "\n",
      "    accuracy = 0.9883714285714286\n",
      "    balanced accuracy = 0.5312919847440607\n",
      "    precision = 0.35071090047393366\n",
      "    recall = 0.06390328151986183\n",
      "    \u001b[1mf1_score = 0.10810810810810809\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207410    274]\n",
      " [  2168    148]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "[LightGBM] [Info] Number of positive: 96919, number of negative: 387676\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3967\n",
      "[LightGBM] [Info] Number of data points in the train set: 484595, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.200000 -> initscore=-1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "\u001b[1mSet Mix Over + Under sampling\u001b[0m\n",
      "model score: 0.988\n",
      "\n",
      "    accuracy = 0.9882095238095238\n",
      "    balanced accuracy = 0.5365471786401681\n",
      "    precision = 0.34189723320158105\n",
      "    recall = 0.07469775474956822\n",
      "    \u001b[1mf1_score = 0.12260807937632885\u001b[0m\n",
      "    confusion matrix:\n",
      "    [[207351    333]\n",
      " [  2143    173]]\n",
      "    ____________________________________________________________________________________________\n",
      "    \n",
      "CPU times: total: 1min 19s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbm = LGBMClassifier()\n",
    "datasets_train = [[X_train_complete, y_train_complete]\\\n",
    "                 ,[X_train_pca, y_train_1]\\\n",
    "                 ,[X_train_under, y_train_under]\\\n",
    "                 ,[X_train_over, y_train_over]\\\n",
    "                 ,[X_train_mix, y_train_mix]\\\n",
    "                 ]\n",
    "\n",
    "datasets_validation = [[X_val_complete, y_val_complete]\\\n",
    "                      ,[X_val_pca, y_val]\\\n",
    "                      ,[X_val, y_val]\\\n",
    "                      ,[X_val, y_val]\\\n",
    "                      ,[X_val, y_val]\\\n",
    "                       ]\n",
    "\n",
    "names = ['Set completo','Set con PCA', 'Set Undersampling', 'Set Oversampling', 'Set Mix Over + Under sampling']\n",
    "\n",
    "for train, val, name in zip(datasets_train, datasets_validation, names):\n",
    "    lgbm.fit(train[0], train[1])   \n",
    "    accuracy = accuracy_score(val[1],lgbm.predict(val[0]))\n",
    "    balanced_acc = balanced_accuracy_score(val[1],lgbm.predict(val[0]))\n",
    "    f1 = f1_score(val[1],lgbm.predict(val[0])) # (y_true, y_pred)\n",
    "    c_matrix = confusion_matrix(val[1],lgbm.predict(val[0]))\n",
    "    precision = precision_score(val[1],lgbm.predict(val[0]))\n",
    "    recall = recall_score(val[1],lgbm.predict(val[0]))\n",
    "    print(f'\\033[1m{name}\\033[0m')\n",
    "    print(\"model score: %.3f\" % lgbm.score(val[0], val[1]))\n",
    "    print(f\"\"\"\n",
    "    accuracy = {accuracy}\n",
    "    balanced accuracy = {balanced_acc}\n",
    "    precision = {precision}\n",
    "    recall = {recall}\n",
    "    \\033[1mf1_score = {f1}\\033[0m\n",
    "    confusion matrix:\n",
    "    {c_matrix}\n",
    "    ____________________________________________________________________________________________\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ad9aa-b89e-4adb-8f6e-ab90e30e640a",
   "metadata": {},
   "source": [
    "### Conclusión de la comparación\n",
    "\n",
    "Realizar esta comparativa resultó ser realmente interesante, ya que son varias las conclusiones que podemos sacar:\n",
    "\n",
    "1. En primer lugar y más importante, es que el set con UnderSampling parece ser el que mejor resultados da, sorprendentemente con gran diferencia vs los demás. He de mencionar que realicé algunas pruebas en cuanto al % de balanceado que se le realizó al dataset, para no abusar de la eliminación de los datos y manetener buenas métricas. Estas pruebas podrán encontrarse en el notebook Experiments de la carpeta Experiments.\n",
    "2. Con mejores resultados me refiero a que aumenta mucho el F1 (0.228) con respecto a los demás manteniendo el Accuracy (0.979). Aún así, ese aumento del F1 se debe sobre todo al aumento del Recall, mientras que la precisión es menor a los datasets con OverSampling. Será interesante ver si con ajustes en los hiperparámetros esta métrica puede mejorar.\n",
    "2. Es interesante recalcar también que el mix de UnderSampling y OverSampling no da tan mal resultado, mejorando al set que solo tiene undersampling y manteniendo una cantidad de datos similar al train base.\n",
    "3. Es interesante también ver como haber recortado algunas variables no solo ha mantenido las métricas del modelo, sino que las ha mejorado. Esto lo podemos observar comparando las métricas del modelo con las variables recortadas (el primero que calculamos en la comparativa de modelos) y las del set completo, donde el primero mejora tanto en precision como en Recall al segundo, con el mismo accuracy y con una menor necesidad de procesamiento por el simple hecho de tener menos atributos.\n",
    "4. También se descarta por completo la idea de que las variables del PCA pudiesen mejorar el modelo, ya que como vemos, parecen simplemente haber incluído ruido innecesario, empeorando las métricas del mismo.\n",
    "\n",
    "Una vez determinado tanto el modelo como el set a utilizar, procederé a llevar a cabo la búsqueda de hiperparámetros. Pero antes de ello, voy a probar el modelo gaussiano de bayes en el dataset con undersampling, para ver si mejora o no sus resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90136172-e4d4-4f45-9378-f1ac07f22230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy = 0.8875\n",
      "balanced accuracy = 0.712348725190612\n",
      "precision = 0.05193658269902014\n",
      "recall = 0.5332469775474957\n",
      "\u001b[1mf1_score = 0.09465414830427285\u001b[0m\n",
      "confusion matrix:\n",
      "[[185140  22544]\n",
      " [  1081   1235]]\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_under, y_train_under)\n",
    "\n",
    "accuracy = accuracy_score(y_val,gnb.predict(X_val))\n",
    "balanced_acc = balanced_accuracy_score(y_val,gnb.predict(X_val))\n",
    "f1 = f1_score(y_val,gnb.predict(X_val)) # (y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_val,gnb.predict(X_val))\n",
    "precision = precision_score(y_val,gnb.predict(X_val))\n",
    "recall = recall_score(y_val,gnb.predict(X_val))\n",
    "\n",
    "print(f\"\"\"\n",
    "accuracy = {accuracy}\n",
    "balanced accuracy = {balanced_acc}\n",
    "precision = {precision}\n",
    "recall = {recall}\n",
    "\\033[1mf1_score = {f1}\\033[0m\n",
    "confusion matrix:\n",
    "{c_matrix}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e930691-fcf0-410e-8e9c-5bc91425adbf",
   "metadata": {},
   "source": [
    "No solo no han mejorado, sino que han empeorado. Veremos que sucede luego de la búsqueda de hiperparámetros del LightGBM, pero parece que lo que\n",
    "pensaba probablemente sea real: el LightGBM puede seguir mejorando, aún más que el GaussianNB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073f36c-25e6-408e-8fa4-36bf60dd7b4a",
   "metadata": {},
   "source": [
    "# <font color='red'>Para hacer:</font>\n",
    "\n",
    "- <s>PCA o T-SNE graficando 2 componentes</s>\n",
    "- <s>TEST DE VAL EN EL 03, antes del oversampling!</s>\n",
    "- <s>Crear variable random y medir en el random forest de selección de variables! (hacer pd.concat con el train y el test!)</s>\n",
    "- <s>Logistic Regression</s>\n",
    "- <s>GLM</s>\n",
    "- <s>Explicar selección de modelos y de datasets</s>\n",
    "- Buscar hiperparámetros con Grid Search en XGBoos y LightGBM\n",
    "- BUSINESS CASE\n",
    "\n",
    "# ORDENAR:\n",
    "\n",
    "Antes que nada: Nuevo directorio con todo igual pero nuevo\n",
    "Nuevo orden de notebooks:\n",
    "1. 01_Introduction_EDA\n",
    "    - **Agregar Business Case**\n",
    "        - **Si mi idea es que el modelo se ejecute ante cada aplicación, el \"day_since_request\" Sería una variable inutilizable!!!** Ver esto!\n",
    "    - <s>**Revisar si cambiamos o no los -1 por -666**</s> --> Por ahora NO --> **Preguntar a Ana**\n",
    "2. 02_EDA_and_Split\n",
    "    - <s>Mantengo igual pero a esto le sumo la conclusión</s>\n",
    "    - <s>**Hacer confusion Matrix como quiere Diego**</s>\n",
    "    - **PASAR LA CONFUSION MÁTRIX A FUNCIÓN**\n",
    "    - <s>**Ver posibilidad de actualizar la Cramers-V Matrix - Recordar actualizar la escala de colores a Divergente!**</s> --> Hay q tocar la función si quiero eso\n",
    "3. 03_Feature_Processing\n",
    "    - Acá va todo lo nuevo:\n",
    "         1. <s>Feature Engineering (Encodings, Scaling y **PCA+kmeans**)</s>\n",
    "         2. <s>Feature Selection</s>\n",
    "             - <s>**Unificar Train y Test acá**</s>\n",
    "             - <s>Random Forest --> Meter y dsps eliminar variable random entre 1 y 1000. Recordar NO eliminar columns del OHE</s>\n",
    "             - <s>Lasso --> Eliminar una column del OHE</s>\n",
    "         3. **Nuevo diccionario de datos!**\n",
    "         4. <s>Exporto todos los DS listos</s>\n",
    "             - <s>Train y Test normales</s>\n",
    "             - <s>Exporto pickle de PCA</s>\n",
    "4. 04_Model_Selection\n",
    "    1. <s>Importo todo:</s>\n",
    "        - <s>Train y Test base</s>\n",
    "        - <s>Train y Test scaled</s>\n",
    "        - <s>**PCA (importando pickle)**</s>\n",
    "    2. <s>Train_Validation split\n",
    "        - Tanto escalado como normal</s>\n",
    "    3. <s>OverSampling y UnderSampling\n",
    "        - 3 posibilidades: Under, Over y Mix</s>\n",
    "    4. <s>Modelo Base --> Sobre el DS Base</s>\n",
    "    5. <s>**Regresión Logística --> Aplicar Scaled**</s>\n",
    "    6. <s>**Pipeline selección del modelo (f1, precision, accuracy, recall, roc_auc y confusion_matrix)**</s>\n",
    "    7. <s>**Pipeline selección dataset (mismas métricas)**\n",
    "        - Dataset original\n",
    "        - Dataset Recortado por featuring selection\n",
    "        - Dataset Recortado + PCA\n",
    "        - UnderSampling\n",
    "        - OverSampling\n",
    "        - MixSamplings</s>\n",
    "    8. **Explicación de la selección del modelo con métricas**\n",
    "5. 05_Model_Fit_and_Metrics\n",
    "     1. Búsqueda de hiperparámetros\n",
    "     2. Fit del modelo y predict\n",
    "     3. Métricas\n",
    "     4. Threshold + Confusion Matrix\n",
    "     5. ROC Curve, Gain Curve, Lift plot y Precision_REcall curve\n",
    "     6. CONCLUSIONES DE LA ELECCIÓN Y PREDICCIÓN\n",
    "6. 06_Explainability? Explicabilidad o interpretabilidad?\n",
    "    - SHAP plots\n",
    "    - SHAP clustering!\n",
    "7. 07_Review_Conclusions\n",
    "\n",
    "# <s>HACER SMV! AL MENOS CON POCOS DATOS!</s>\n",
    "\n",
    "## Revisar todo y ver que se puede pasar a FUNCIONES!\n",
    "- Confusion Matrix por ejemplo!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2855f-efb2-4136-afe1-7b9f28cf7627",
   "metadata": {},
   "source": [
    "## Búsqueda de hiperparámetros\n",
    "\n",
    "En la <a href='https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html'>documentación de LightGBM</a> se dan algunos consejos sobre el tunning de los hiperparámetros del modelo, los cuales por cierto pueden ser muchos más.\n",
    "\n",
    "Yo me he dedicado a comprobar entre los principales nombrados, siguiendo algunas indicaciones para determinar los valores elegidos, intentando que no permitan el overfitting ni underfitting. Creo que los parametros elegidos se adaptan bien al tamaño del dataset y permiten una buena variedad de opciones en el cálculo.\n",
    "\n",
    "Los mismos serán evaluados con un Random Search, el cual normalmente obtiene resultados similares a un Grid Search pero con tiempos de procesamiento menores.\n",
    "\n",
    "Al ya haber seleccionado el modelo y el dataset a utilizar finalmente, ésta búsqueda de hiperparámetros se realizará en un dataset que combina tanto el train como el validation set, logrando así utilizar una mayor cantidad de datos en el análisis, debido a que la comparación ahora será contra el set de Test. Una vez obtenidos los hiperparámetros, la implementación del modelo y sus medidas se realizarán en un siguiente notebook aparte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b8eaa-9b1a-45b4-8d80-82325c902f8e",
   "metadata": {},
   "source": [
    "#### Vuelvo a combinar Test y Validation y realizo Undersampling, obteniendo el set de Train base para este análisis (X_train_2 e y_train_2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7372a258-e3f8-49cd-908c-9d8e84e9a187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104220, 29),\n",
       " fraud_bool\n",
       " 0    0.925926\n",
       " 1    0.074074\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersamp = RandomUnderSampler(sampling_strategy=0.08, random_state=seed)\n",
    "X_train_2, y_train_2 = undersamp.fit_resample(X_train_0, y_train_0)\n",
    "\n",
    "X_train_2.shape, y_train_2.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd33d5a-940b-40fe-b3e7-eb9406800d86",
   "metadata": {},
   "source": [
    "Obtenemos un dataset de train de 104220 instancias y los 29 atributos, en este caso con un poco menos de desbalanceo que el set original.\n",
    "\n",
    "Con este dataset procederemos a realizar la búsqueda de hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "aab38142-2c09-4c18-90f7-233a39888cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(2**7/1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "77094e7e-f80b-4050-8cbf-f4e50057f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: 500\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1600\n",
      "[LightGBM] [Warning] num_iterations is set=100, n_estimators= will be ignored. Current value: num_iterations=100\n",
      "[LightGBM] [Warning] Unknown parameter: 500\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1600\n",
      "[LightGBM] [Warning] num_iterations is set=100, n_estimators= will be ignored. Current value: num_iterations=100\n",
      "[LightGBM] [Info] Number of positive: 7720, number of negative: 96500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2670\n",
      "[LightGBM] [Info] Number of data points in the train set: 104220, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074074 -> initscore=-2.525729\n",
      "[LightGBM] [Info] Start training from score -2.525729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "{'num_leaves': 171, 'n_estimators ': 500, 'min_data_in_leaf': 1600, 'max_depth': 10, 'learning_rate': 0.05}\n",
      "0.26757177150852196\n",
      "CPU times: total: 22.3 s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "param_grid = { \n",
    "    'max_depth' : [8,9,10]\n",
    "    ,'num_leaves' : [round(2**8/1.5), round(2**9/1.5), round(2**10/1.5)] # La documentación de LightGBM comenta que num_leaves se relaciona con el max_depth así: (num_leaves = (2^max_depth/1.5))\n",
    "    ,'min_data_in_leaf' :[1400,1600,1800]\n",
    "    ,'n_estimators ': [500,600,700]\n",
    "    ,'learning_rate': [0.01, 0.02, 0.05]\n",
    "}\n",
    "\n",
    "scoring = {'f2':f2_scorer,'f1':'f1','accuracy':'accuracy', 'balanced_accuracy':'balanced_accuracy', 'precision':'precision', 'recall':'recall','roc_auc':'roc_auc'}\n",
    "\n",
    "CV_rs = RandomizedSearchCV(lgbm, param_grid, cv=10, random_state=seed, n_jobs=2, scoring=scoring, refit='f2')\n",
    "\n",
    "CV_rs.fit(X_train_2, y_train_2)  \n",
    "print(CV_rs.best_params_)    \n",
    "print(CV_rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "2e3846ee-7ebb-46a3-b8ca-6a15b0c1496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-61 {color: black;}#sk-container-id-61 pre{padding: 0;}#sk-container-id-61 div.sk-toggleable {background-color: white;}#sk-container-id-61 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-61 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-61 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-61 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-61 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-61 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-61 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-61 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-61 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-61 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-61 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-61 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-61 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-61 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-61 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-61 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-61 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-61 div.sk-item {position: relative;z-index: 1;}#sk-container-id-61 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-61 div.sk-item::before, #sk-container-id-61 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-61 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-61 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-61 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-61 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-61 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-61 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-61 div.sk-label-container {text-align: center;}#sk-container-id-61 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-61 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-61\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.05, max_depth=10, min_data_in_leaf=1600,\n",
       "               n_estimators =500, num_leaves=171)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" checked><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.05, max_depth=10, min_data_in_leaf=1600,\n",
       "               n_estimators =500, num_leaves=171)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.05, max_depth=10, min_data_in_leaf=1600,\n",
       "               n_estimators =500, num_leaves=171)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "299d00b9-810c-4668-a34d-8218fa2376fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 171,\n",
       " 'n_estimators ': 500,\n",
       " 'min_data_in_leaf': 1600,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.05}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "3a2a2e16-de71-4a93-a2fa-51fb2453403c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26757177150852196"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "4cf5bc40-69a0-4937-897c-b299ecc754e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_accuracy: \n",
      "[0.92592593 0.9355498  0.93558818 0.93048359 0.93010938 0.93578968\n",
      " 0.92592593 0.93578968 0.93020533 0.93010938]\n",
      "mean_test_f2: \n",
      "[0.         0.26757177 0.26010976 0.10594228 0.09735704 0.2667072\n",
      " 0.         0.2667072  0.09875081 0.09735704]\n",
      "mean_test_f1: \n",
      "[0.         0.34755677 0.34053778 0.15648377 0.14472038 0.34740763\n",
      " 0.         0.34740763 0.14668659 0.14472038]\n",
      "mean_test_precision: \n",
      "[0.         0.69421946 0.70455658 0.77389865 0.77367259 0.70273589\n",
      " 0.         0.70273589 0.77707901 0.77367259]\n",
      "mean_test_recall: \n",
      "[0.         0.23199482 0.22474093 0.08717617 0.07992228 0.23095855\n",
      " 0.         0.23095855 0.08108808 0.07992228]\n",
      "mean_test_roc_auc: \n",
      "[0.86262788 0.89192921 0.8913475  0.87978758 0.87950179 0.89150153\n",
      " 0.86367409 0.89150153 0.87975967 0.87950179]\n"
     ]
    }
   ],
   "source": [
    "metrics = ['mean_test_accuracy','mean_test_f2','mean_test_f1','mean_test_precision','mean_test_recall','mean_test_roc_auc']\n",
    "for i in metrics:\n",
    "    print(f'{i}: \\n{CV_rs.cv_results_[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ecdd5129-5596-49da-95ee-9fc3ba127969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rs.cv_results_['rank_test_accuracy'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "dcde036e-b69e-43a1-8595-3314269426a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "mean_test_accuracy: 0.93011\n",
      "mean_test_f2: 0.09736\n",
      "mean_test_f1: 0.14472\n",
      "mean_test_precision: 0.77367\n",
      "mean_test_recall: 0.07992\n",
      "mean_test_roc_auc: 0.87950\n",
      "1:\n",
      "mean_test_accuracy: 0.92593\n",
      "mean_test_f2: 0.00000\n",
      "mean_test_f1: 0.00000\n",
      "mean_test_precision: 0.00000\n",
      "mean_test_recall: 0.00000\n",
      "mean_test_roc_auc: 0.86263\n",
      "2:\n",
      "mean_test_accuracy: 0.93555\n",
      "mean_test_f2: 0.26757\n",
      "mean_test_f1: 0.34756\n",
      "mean_test_precision: 0.69422\n",
      "mean_test_recall: 0.23199\n",
      "mean_test_roc_auc: 0.89193\n",
      "3:\n",
      "mean_test_accuracy: 0.93559\n",
      "mean_test_f2: 0.26011\n",
      "mean_test_f1: 0.34054\n",
      "mean_test_precision: 0.70456\n",
      "mean_test_recall: 0.22474\n",
      "mean_test_roc_auc: 0.89135\n",
      "4:\n",
      "mean_test_accuracy: 0.93048\n",
      "mean_test_f2: 0.10594\n",
      "mean_test_f1: 0.15648\n",
      "mean_test_precision: 0.77390\n",
      "mean_test_recall: 0.08718\n",
      "mean_test_roc_auc: 0.87979\n",
      "5:\n",
      "mean_test_accuracy: 0.93011\n",
      "mean_test_f2: 0.09736\n",
      "mean_test_f1: 0.14472\n",
      "mean_test_precision: 0.77367\n",
      "mean_test_recall: 0.07992\n",
      "mean_test_roc_auc: 0.87950\n",
      "6:\n",
      "mean_test_accuracy: 0.93579\n",
      "mean_test_f2: 0.26671\n",
      "mean_test_f1: 0.34741\n",
      "mean_test_precision: 0.70274\n",
      "mean_test_recall: 0.23096\n",
      "mean_test_roc_auc: 0.89150\n",
      "7:\n",
      "mean_test_accuracy: 0.92593\n",
      "mean_test_f2: 0.00000\n",
      "mean_test_f1: 0.00000\n",
      "mean_test_precision: 0.00000\n",
      "mean_test_recall: 0.00000\n",
      "mean_test_roc_auc: 0.86367\n",
      "8:\n",
      "mean_test_accuracy: 0.93579\n",
      "mean_test_f2: 0.26671\n",
      "mean_test_f1: 0.34741\n",
      "mean_test_precision: 0.70274\n",
      "mean_test_recall: 0.23096\n",
      "mean_test_roc_auc: 0.89150\n",
      "9:\n",
      "mean_test_accuracy: 0.93021\n",
      "mean_test_f2: 0.09875\n",
      "mean_test_f1: 0.14669\n",
      "mean_test_precision: 0.77708\n",
      "mean_test_recall: 0.08109\n",
      "mean_test_roc_auc: 0.87976\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f'{i}:')\n",
    "    for metric in metrics:\n",
    "        print(f'{metric}: {CV_rs.cv_results_[metric][i-1]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5d94b619-9ed6-4585-9ff6-7f10fac48df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_data_in_leaf</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_f2</th>\n",
       "      <th>split1_test_f2</th>\n",
       "      <th>split2_test_f2</th>\n",
       "      <th>split3_test_f2</th>\n",
       "      <th>split4_test_f2</th>\n",
       "      <th>split5_test_f2</th>\n",
       "      <th>split6_test_f2</th>\n",
       "      <th>split7_test_f2</th>\n",
       "      <th>split8_test_f2</th>\n",
       "      <th>split9_test_f2</th>\n",
       "      <th>mean_test_f2</th>\n",
       "      <th>std_test_f2</th>\n",
       "      <th>rank_test_f2</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>split3_test_f1</th>\n",
       "      <th>split4_test_f1</th>\n",
       "      <th>split5_test_f1</th>\n",
       "      <th>split6_test_f1</th>\n",
       "      <th>split7_test_f1</th>\n",
       "      <th>split8_test_f1</th>\n",
       "      <th>split9_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>split4_test_accuracy</th>\n",
       "      <th>split5_test_accuracy</th>\n",
       "      <th>split6_test_accuracy</th>\n",
       "      <th>split7_test_accuracy</th>\n",
       "      <th>split8_test_accuracy</th>\n",
       "      <th>split9_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>split0_test_balanced_accuracy</th>\n",
       "      <th>split1_test_balanced_accuracy</th>\n",
       "      <th>split2_test_balanced_accuracy</th>\n",
       "      <th>split3_test_balanced_accuracy</th>\n",
       "      <th>split4_test_balanced_accuracy</th>\n",
       "      <th>split5_test_balanced_accuracy</th>\n",
       "      <th>split6_test_balanced_accuracy</th>\n",
       "      <th>split7_test_balanced_accuracy</th>\n",
       "      <th>split8_test_balanced_accuracy</th>\n",
       "      <th>split9_test_balanced_accuracy</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>std_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>split3_test_precision</th>\n",
       "      <th>split4_test_precision</th>\n",
       "      <th>split5_test_precision</th>\n",
       "      <th>split6_test_precision</th>\n",
       "      <th>split7_test_precision</th>\n",
       "      <th>split8_test_precision</th>\n",
       "      <th>split9_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>split0_test_recall</th>\n",
       "      <th>split1_test_recall</th>\n",
       "      <th>split2_test_recall</th>\n",
       "      <th>split3_test_recall</th>\n",
       "      <th>split4_test_recall</th>\n",
       "      <th>split5_test_recall</th>\n",
       "      <th>split6_test_recall</th>\n",
       "      <th>split7_test_recall</th>\n",
       "      <th>split8_test_recall</th>\n",
       "      <th>split9_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <th>split3_test_roc_auc</th>\n",
       "      <th>split4_test_roc_auc</th>\n",
       "      <th>split5_test_roc_auc</th>\n",
       "      <th>split6_test_roc_auc</th>\n",
       "      <th>split7_test_roc_auc</th>\n",
       "      <th>split8_test_roc_auc</th>\n",
       "      <th>split9_test_roc_auc</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.846054</td>\n",
       "      <td>0.251690</td>\n",
       "      <td>0.198615</td>\n",
       "      <td>0.023320</td>\n",
       "      <td>341</td>\n",
       "      <td>500</td>\n",
       "      <td>1800</td>\n",
       "      <td>9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'num_leaves': 341, 'n_estimators ': 500, 'min...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.859600</td>\n",
       "      <td>0.870019</td>\n",
       "      <td>0.857176</td>\n",
       "      <td>0.863115</td>\n",
       "      <td>0.861620</td>\n",
       "      <td>0.859452</td>\n",
       "      <td>0.868315</td>\n",
       "      <td>0.864777</td>\n",
       "      <td>0.860631</td>\n",
       "      <td>0.861574</td>\n",
       "      <td>0.862628</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.812730</td>\n",
       "      <td>0.162282</td>\n",
       "      <td>0.242726</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>171</td>\n",
       "      <td>500</td>\n",
       "      <td>1600</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'num_leaves': 171, 'n_estimators ': 500, 'min...</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>0.280012</td>\n",
       "      <td>0.226244</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.281899</td>\n",
       "      <td>0.258595</td>\n",
       "      <td>0.268978</td>\n",
       "      <td>0.273297</td>\n",
       "      <td>0.269461</td>\n",
       "      <td>0.256834</td>\n",
       "      <td>0.267572</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375120</td>\n",
       "      <td>0.361191</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.360531</td>\n",
       "      <td>0.336249</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>0.354651</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.337611</td>\n",
       "      <td>0.347557</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937344</td>\n",
       "      <td>0.936193</td>\n",
       "      <td>0.932930</td>\n",
       "      <td>0.935521</td>\n",
       "      <td>0.935329</td>\n",
       "      <td>0.934466</td>\n",
       "      <td>0.935713</td>\n",
       "      <td>0.936097</td>\n",
       "      <td>0.936289</td>\n",
       "      <td>0.935617</td>\n",
       "      <td>0.935550</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>4</td>\n",
       "      <td>0.622953</td>\n",
       "      <td>0.617565</td>\n",
       "      <td>0.593161</td>\n",
       "      <td>0.612435</td>\n",
       "      <td>0.618290</td>\n",
       "      <td>0.607694</td>\n",
       "      <td>0.612539</td>\n",
       "      <td>0.614534</td>\n",
       "      <td>0.612850</td>\n",
       "      <td>0.607124</td>\n",
       "      <td>0.611915</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>1</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.698885</td>\n",
       "      <td>0.660793</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.673152</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.703846</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.709544</td>\n",
       "      <td>0.694219</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>8</td>\n",
       "      <td>0.253886</td>\n",
       "      <td>0.243523</td>\n",
       "      <td>0.194301</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.246114</td>\n",
       "      <td>0.224093</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.237047</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.221503</td>\n",
       "      <td>0.231995</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>1</td>\n",
       "      <td>0.890980</td>\n",
       "      <td>0.898645</td>\n",
       "      <td>0.884923</td>\n",
       "      <td>0.891760</td>\n",
       "      <td>0.892111</td>\n",
       "      <td>0.889440</td>\n",
       "      <td>0.893558</td>\n",
       "      <td>0.895551</td>\n",
       "      <td>0.890260</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.541737</td>\n",
       "      <td>0.146673</td>\n",
       "      <td>0.238994</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>171</td>\n",
       "      <td>500</td>\n",
       "      <td>1800</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'num_leaves': 171, 'n_estimators ': 500, 'min...</td>\n",
       "      <td>0.286653</td>\n",
       "      <td>0.264970</td>\n",
       "      <td>0.225689</td>\n",
       "      <td>0.262369</td>\n",
       "      <td>0.272321</td>\n",
       "      <td>0.248046</td>\n",
       "      <td>0.258827</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>0.271446</td>\n",
       "      <td>0.246988</td>\n",
       "      <td>0.260110</td>\n",
       "      <td>0.015901</td>\n",
       "      <td>4</td>\n",
       "      <td>0.371733</td>\n",
       "      <td>0.345703</td>\n",
       "      <td>0.302538</td>\n",
       "      <td>0.343474</td>\n",
       "      <td>0.350575</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>0.337232</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.355599</td>\n",
       "      <td>0.326693</td>\n",
       "      <td>0.340538</td>\n",
       "      <td>0.017872</td>\n",
       "      <td>4</td>\n",
       "      <td>0.937728</td>\n",
       "      <td>0.935713</td>\n",
       "      <td>0.934082</td>\n",
       "      <td>0.935809</td>\n",
       "      <td>0.934945</td>\n",
       "      <td>0.934753</td>\n",
       "      <td>0.934753</td>\n",
       "      <td>0.935905</td>\n",
       "      <td>0.937056</td>\n",
       "      <td>0.935137</td>\n",
       "      <td>0.935588</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>3</td>\n",
       "      <td>0.620777</td>\n",
       "      <td>0.610751</td>\n",
       "      <td>0.593187</td>\n",
       "      <td>0.609611</td>\n",
       "      <td>0.613912</td>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.610259</td>\n",
       "      <td>0.613860</td>\n",
       "      <td>0.602694</td>\n",
       "      <td>0.608598</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>4</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.699531</td>\n",
       "      <td>0.708502</td>\n",
       "      <td>0.672794</td>\n",
       "      <td>0.693277</td>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.704557</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>5</td>\n",
       "      <td>0.248705</td>\n",
       "      <td>0.229275</td>\n",
       "      <td>0.193005</td>\n",
       "      <td>0.226684</td>\n",
       "      <td>0.237047</td>\n",
       "      <td>0.213731</td>\n",
       "      <td>0.224093</td>\n",
       "      <td>0.227979</td>\n",
       "      <td>0.234456</td>\n",
       "      <td>0.212435</td>\n",
       "      <td>0.224741</td>\n",
       "      <td>0.014612</td>\n",
       "      <td>4</td>\n",
       "      <td>0.890408</td>\n",
       "      <td>0.897350</td>\n",
       "      <td>0.885495</td>\n",
       "      <td>0.891843</td>\n",
       "      <td>0.891794</td>\n",
       "      <td>0.887883</td>\n",
       "      <td>0.894213</td>\n",
       "      <td>0.894413</td>\n",
       "      <td>0.889932</td>\n",
       "      <td>0.890145</td>\n",
       "      <td>0.891347</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.145438</td>\n",
       "      <td>0.249771</td>\n",
       "      <td>0.230443</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>683</td>\n",
       "      <td>700</td>\n",
       "      <td>1400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'num_leaves': 683, 'n_estimators ': 700, 'min...</td>\n",
       "      <td>0.108661</td>\n",
       "      <td>0.097792</td>\n",
       "      <td>0.097792</td>\n",
       "      <td>0.104232</td>\n",
       "      <td>0.119197</td>\n",
       "      <td>0.096275</td>\n",
       "      <td>0.112888</td>\n",
       "      <td>0.133020</td>\n",
       "      <td>0.099432</td>\n",
       "      <td>0.090133</td>\n",
       "      <td>0.105942</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>5</td>\n",
       "      <td>0.160652</td>\n",
       "      <td>0.145199</td>\n",
       "      <td>0.145199</td>\n",
       "      <td>0.155294</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.143192</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0.193402</td>\n",
       "      <td>0.147887</td>\n",
       "      <td>0.134752</td>\n",
       "      <td>0.156484</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>5</td>\n",
       "      <td>0.930819</td>\n",
       "      <td>0.929956</td>\n",
       "      <td>0.929956</td>\n",
       "      <td>0.931107</td>\n",
       "      <td>0.930915</td>\n",
       "      <td>0.929956</td>\n",
       "      <td>0.930052</td>\n",
       "      <td>0.931971</td>\n",
       "      <td>0.930340</td>\n",
       "      <td>0.929764</td>\n",
       "      <td>0.930484</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>5</td>\n",
       "      <td>0.543756</td>\n",
       "      <td>0.539119</td>\n",
       "      <td>0.539119</td>\n",
       "      <td>0.542124</td>\n",
       "      <td>0.547979</td>\n",
       "      <td>0.538523</td>\n",
       "      <td>0.545130</td>\n",
       "      <td>0.553912</td>\n",
       "      <td>0.539922</td>\n",
       "      <td>0.536036</td>\n",
       "      <td>0.542562</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>5</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.773899</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>2</td>\n",
       "      <td>0.089378</td>\n",
       "      <td>0.080311</td>\n",
       "      <td>0.080311</td>\n",
       "      <td>0.085492</td>\n",
       "      <td>0.098446</td>\n",
       "      <td>0.079016</td>\n",
       "      <td>0.093264</td>\n",
       "      <td>0.110104</td>\n",
       "      <td>0.081606</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.087176</td>\n",
       "      <td>0.010331</td>\n",
       "      <td>5</td>\n",
       "      <td>0.877668</td>\n",
       "      <td>0.886890</td>\n",
       "      <td>0.872804</td>\n",
       "      <td>0.879066</td>\n",
       "      <td>0.880288</td>\n",
       "      <td>0.876951</td>\n",
       "      <td>0.884530</td>\n",
       "      <td>0.883032</td>\n",
       "      <td>0.878211</td>\n",
       "      <td>0.878437</td>\n",
       "      <td>0.879788</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.805220</td>\n",
       "      <td>0.143632</td>\n",
       "      <td>0.235149</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>341</td>\n",
       "      <td>700</td>\n",
       "      <td>1600</td>\n",
       "      <td>9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'num_leaves': 341, 'n_estimators ': 700, 'min...</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.093119</td>\n",
       "      <td>0.088552</td>\n",
       "      <td>0.099558</td>\n",
       "      <td>0.108627</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.100787</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.093295</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.097357</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>7</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.138498</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.148585</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.134434</td>\n",
       "      <td>0.149010</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.139480</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>0.144720</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>7</td>\n",
       "      <td>0.930052</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.930723</td>\n",
       "      <td>0.930723</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.929860</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.930148</td>\n",
       "      <td>0.929476</td>\n",
       "      <td>0.930109</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>7</td>\n",
       "      <td>0.539767</td>\n",
       "      <td>0.537124</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.540130</td>\n",
       "      <td>0.543705</td>\n",
       "      <td>0.535933</td>\n",
       "      <td>0.540259</td>\n",
       "      <td>0.548834</td>\n",
       "      <td>0.537435</td>\n",
       "      <td>0.531710</td>\n",
       "      <td>0.539023</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>7</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.773673</td>\n",
       "      <td>0.028884</td>\n",
       "      <td>3</td>\n",
       "      <td>0.081606</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.072539</td>\n",
       "      <td>0.081606</td>\n",
       "      <td>0.089378</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>0.099741</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.064767</td>\n",
       "      <td>0.079922</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>7</td>\n",
       "      <td>0.877155</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.872164</td>\n",
       "      <td>0.879435</td>\n",
       "      <td>0.879533</td>\n",
       "      <td>0.876676</td>\n",
       "      <td>0.885140</td>\n",
       "      <td>0.882417</td>\n",
       "      <td>0.877532</td>\n",
       "      <td>0.878530</td>\n",
       "      <td>0.879502</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.722345</td>\n",
       "      <td>0.089680</td>\n",
       "      <td>0.227398</td>\n",
       "      <td>0.030030</td>\n",
       "      <td>341</td>\n",
       "      <td>700</td>\n",
       "      <td>1400</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'num_leaves': 341, 'n_estimators ': 700, 'min...</td>\n",
       "      <td>0.284820</td>\n",
       "      <td>0.275037</td>\n",
       "      <td>0.226998</td>\n",
       "      <td>0.273461</td>\n",
       "      <td>0.274889</td>\n",
       "      <td>0.260948</td>\n",
       "      <td>0.268336</td>\n",
       "      <td>0.274955</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.255255</td>\n",
       "      <td>0.266707</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>2</td>\n",
       "      <td>0.368370</td>\n",
       "      <td>0.357629</td>\n",
       "      <td>0.303644</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>0.352717</td>\n",
       "      <td>0.341847</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.357282</td>\n",
       "      <td>0.355122</td>\n",
       "      <td>0.335306</td>\n",
       "      <td>0.347408</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>2</td>\n",
       "      <td>0.937152</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.933986</td>\n",
       "      <td>0.936289</td>\n",
       "      <td>0.934849</td>\n",
       "      <td>0.935713</td>\n",
       "      <td>0.934945</td>\n",
       "      <td>0.936481</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.935329</td>\n",
       "      <td>0.935790</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619870</td>\n",
       "      <td>0.615389</td>\n",
       "      <td>0.593731</td>\n",
       "      <td>0.614637</td>\n",
       "      <td>0.615052</td>\n",
       "      <td>0.608964</td>\n",
       "      <td>0.612124</td>\n",
       "      <td>0.615337</td>\n",
       "      <td>0.614197</td>\n",
       "      <td>0.606373</td>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>2</td>\n",
       "      <td>0.720755</td>\n",
       "      <td>0.715953</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.667870</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.719368</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.702736</td>\n",
       "      <td>0.017064</td>\n",
       "      <td>6</td>\n",
       "      <td>0.247409</td>\n",
       "      <td>0.238342</td>\n",
       "      <td>0.194301</td>\n",
       "      <td>0.237047</td>\n",
       "      <td>0.239637</td>\n",
       "      <td>0.225389</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.238342</td>\n",
       "      <td>0.235751</td>\n",
       "      <td>0.220207</td>\n",
       "      <td>0.230959</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>2</td>\n",
       "      <td>0.890648</td>\n",
       "      <td>0.898003</td>\n",
       "      <td>0.884790</td>\n",
       "      <td>0.891769</td>\n",
       "      <td>0.893373</td>\n",
       "      <td>0.888339</td>\n",
       "      <td>0.892263</td>\n",
       "      <td>0.895103</td>\n",
       "      <td>0.891046</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>0.891502</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.245672</td>\n",
       "      <td>0.200463</td>\n",
       "      <td>0.220983</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>683</td>\n",
       "      <td>600</td>\n",
       "      <td>1600</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'num_leaves': 683, 'n_estimators ': 600, 'min...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.860678</td>\n",
       "      <td>0.872161</td>\n",
       "      <td>0.858087</td>\n",
       "      <td>0.864011</td>\n",
       "      <td>0.862688</td>\n",
       "      <td>0.860546</td>\n",
       "      <td>0.869442</td>\n",
       "      <td>0.865066</td>\n",
       "      <td>0.861599</td>\n",
       "      <td>0.862464</td>\n",
       "      <td>0.863674</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.838086</td>\n",
       "      <td>0.208058</td>\n",
       "      <td>0.271301</td>\n",
       "      <td>0.010580</td>\n",
       "      <td>683</td>\n",
       "      <td>700</td>\n",
       "      <td>1400</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'num_leaves': 683, 'n_estimators ': 700, 'min...</td>\n",
       "      <td>0.284820</td>\n",
       "      <td>0.275037</td>\n",
       "      <td>0.226998</td>\n",
       "      <td>0.273461</td>\n",
       "      <td>0.274889</td>\n",
       "      <td>0.260948</td>\n",
       "      <td>0.268336</td>\n",
       "      <td>0.274955</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.255255</td>\n",
       "      <td>0.266707</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>2</td>\n",
       "      <td>0.368370</td>\n",
       "      <td>0.357629</td>\n",
       "      <td>0.303644</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>0.352717</td>\n",
       "      <td>0.341847</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.357282</td>\n",
       "      <td>0.355122</td>\n",
       "      <td>0.335306</td>\n",
       "      <td>0.347408</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>2</td>\n",
       "      <td>0.937152</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.933986</td>\n",
       "      <td>0.936289</td>\n",
       "      <td>0.934849</td>\n",
       "      <td>0.935713</td>\n",
       "      <td>0.934945</td>\n",
       "      <td>0.936481</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.935329</td>\n",
       "      <td>0.935790</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619870</td>\n",
       "      <td>0.615389</td>\n",
       "      <td>0.593731</td>\n",
       "      <td>0.614637</td>\n",
       "      <td>0.615052</td>\n",
       "      <td>0.608964</td>\n",
       "      <td>0.612124</td>\n",
       "      <td>0.615337</td>\n",
       "      <td>0.614197</td>\n",
       "      <td>0.606373</td>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>2</td>\n",
       "      <td>0.720755</td>\n",
       "      <td>0.715953</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.667870</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.719368</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.702736</td>\n",
       "      <td>0.017064</td>\n",
       "      <td>6</td>\n",
       "      <td>0.247409</td>\n",
       "      <td>0.238342</td>\n",
       "      <td>0.194301</td>\n",
       "      <td>0.237047</td>\n",
       "      <td>0.239637</td>\n",
       "      <td>0.225389</td>\n",
       "      <td>0.233161</td>\n",
       "      <td>0.238342</td>\n",
       "      <td>0.235751</td>\n",
       "      <td>0.220207</td>\n",
       "      <td>0.230959</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>2</td>\n",
       "      <td>0.890648</td>\n",
       "      <td>0.898003</td>\n",
       "      <td>0.884790</td>\n",
       "      <td>0.891769</td>\n",
       "      <td>0.893373</td>\n",
       "      <td>0.888339</td>\n",
       "      <td>0.892263</td>\n",
       "      <td>0.895103</td>\n",
       "      <td>0.891046</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>0.891502</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.804227</td>\n",
       "      <td>0.065136</td>\n",
       "      <td>0.243845</td>\n",
       "      <td>0.015710</td>\n",
       "      <td>683</td>\n",
       "      <td>500</td>\n",
       "      <td>1600</td>\n",
       "      <td>10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'num_leaves': 683, 'n_estimators ': 500, 'min...</td>\n",
       "      <td>0.097792</td>\n",
       "      <td>0.093148</td>\n",
       "      <td>0.091627</td>\n",
       "      <td>0.099558</td>\n",
       "      <td>0.111670</td>\n",
       "      <td>0.090104</td>\n",
       "      <td>0.105412</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.094847</td>\n",
       "      <td>0.080901</td>\n",
       "      <td>0.098751</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>6</td>\n",
       "      <td>0.145199</td>\n",
       "      <td>0.138660</td>\n",
       "      <td>0.136631</td>\n",
       "      <td>0.148585</td>\n",
       "      <td>0.164542</td>\n",
       "      <td>0.134593</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.179517</td>\n",
       "      <td>0.141677</td>\n",
       "      <td>0.122010</td>\n",
       "      <td>0.146687</td>\n",
       "      <td>0.015573</td>\n",
       "      <td>6</td>\n",
       "      <td>0.929956</td>\n",
       "      <td>0.929668</td>\n",
       "      <td>0.929668</td>\n",
       "      <td>0.930723</td>\n",
       "      <td>0.930819</td>\n",
       "      <td>0.929668</td>\n",
       "      <td>0.930148</td>\n",
       "      <td>0.931587</td>\n",
       "      <td>0.930244</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.930205</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>6</td>\n",
       "      <td>0.539119</td>\n",
       "      <td>0.537176</td>\n",
       "      <td>0.536580</td>\n",
       "      <td>0.540130</td>\n",
       "      <td>0.544948</td>\n",
       "      <td>0.535984</td>\n",
       "      <td>0.542202</td>\n",
       "      <td>0.549534</td>\n",
       "      <td>0.538083</td>\n",
       "      <td>0.532358</td>\n",
       "      <td>0.539611</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>6</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.027620</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080311</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.075130</td>\n",
       "      <td>0.081606</td>\n",
       "      <td>0.091969</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.086788</td>\n",
       "      <td>0.101036</td>\n",
       "      <td>0.077720</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>0.009469</td>\n",
       "      <td>6</td>\n",
       "      <td>0.877277</td>\n",
       "      <td>0.886623</td>\n",
       "      <td>0.872615</td>\n",
       "      <td>0.879823</td>\n",
       "      <td>0.879883</td>\n",
       "      <td>0.877626</td>\n",
       "      <td>0.885298</td>\n",
       "      <td>0.882565</td>\n",
       "      <td>0.877136</td>\n",
       "      <td>0.878750</td>\n",
       "      <td>0.879760</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.763606</td>\n",
       "      <td>0.192805</td>\n",
       "      <td>0.245038</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>683</td>\n",
       "      <td>500</td>\n",
       "      <td>1600</td>\n",
       "      <td>9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'num_leaves': 683, 'n_estimators ': 500, 'min...</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.093119</td>\n",
       "      <td>0.088552</td>\n",
       "      <td>0.099558</td>\n",
       "      <td>0.108627</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.100787</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.093295</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.097357</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>7</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.138498</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.148585</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.134434</td>\n",
       "      <td>0.149010</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.139480</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>0.144720</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>7</td>\n",
       "      <td>0.930052</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.930723</td>\n",
       "      <td>0.930723</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.929860</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.930148</td>\n",
       "      <td>0.929476</td>\n",
       "      <td>0.930109</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>7</td>\n",
       "      <td>0.539767</td>\n",
       "      <td>0.537124</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.540130</td>\n",
       "      <td>0.543705</td>\n",
       "      <td>0.535933</td>\n",
       "      <td>0.540259</td>\n",
       "      <td>0.548834</td>\n",
       "      <td>0.537435</td>\n",
       "      <td>0.531710</td>\n",
       "      <td>0.539023</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>7</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.773673</td>\n",
       "      <td>0.028884</td>\n",
       "      <td>3</td>\n",
       "      <td>0.081606</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.072539</td>\n",
       "      <td>0.081606</td>\n",
       "      <td>0.089378</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>0.099741</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.064767</td>\n",
       "      <td>0.079922</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>7</td>\n",
       "      <td>0.877155</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.872164</td>\n",
       "      <td>0.879435</td>\n",
       "      <td>0.879533</td>\n",
       "      <td>0.876676</td>\n",
       "      <td>0.885140</td>\n",
       "      <td>0.882417</td>\n",
       "      <td>0.877532</td>\n",
       "      <td>0.878530</td>\n",
       "      <td>0.879502</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.846054      0.251690         0.198615        0.023320   \n",
       "1       2.812730      0.162282         0.242726        0.019114   \n",
       "2       2.541737      0.146673         0.238994        0.016100   \n",
       "3       3.145438      0.249771         0.230443        0.026688   \n",
       "4       2.805220      0.143632         0.235149        0.027691   \n",
       "5       2.722345      0.089680         0.227398        0.030030   \n",
       "6       3.245672      0.200463         0.220983        0.021782   \n",
       "7       2.838086      0.208058         0.271301        0.010580   \n",
       "8       2.804227      0.065136         0.243845        0.015710   \n",
       "9       2.763606      0.192805         0.245038        0.015744   \n",
       "\n",
       "  param_num_leaves param_n_estimators  param_min_data_in_leaf param_max_depth  \\\n",
       "0              341                 500                   1800               9   \n",
       "1              171                 500                   1600              10   \n",
       "2              171                 500                   1800               8   \n",
       "3              683                 700                   1400               9   \n",
       "4              341                 700                   1600               9   \n",
       "5              341                 700                   1400               8   \n",
       "6              683                 600                   1600              10   \n",
       "7              683                 700                   1400               8   \n",
       "8              683                 500                   1600              10   \n",
       "9              683                 500                   1600               9   \n",
       "\n",
       "  param_learning_rate                                             params  \\\n",
       "0                0.01  {'num_leaves': 341, 'n_estimators ': 500, 'min...   \n",
       "1                0.05  {'num_leaves': 171, 'n_estimators ': 500, 'min...   \n",
       "2                0.05  {'num_leaves': 171, 'n_estimators ': 500, 'min...   \n",
       "3                0.02  {'num_leaves': 683, 'n_estimators ': 700, 'min...   \n",
       "4                0.02  {'num_leaves': 341, 'n_estimators ': 700, 'min...   \n",
       "5                0.05  {'num_leaves': 341, 'n_estimators ': 700, 'min...   \n",
       "6                0.01  {'num_leaves': 683, 'n_estimators ': 600, 'min...   \n",
       "7                0.05  {'num_leaves': 683, 'n_estimators ': 700, 'min...   \n",
       "8                0.02  {'num_leaves': 683, 'n_estimators ': 500, 'min...   \n",
       "9                0.02  {'num_leaves': 683, 'n_estimators ': 500, 'min...   \n",
       "\n",
       "   split0_test_f2  split1_test_f2  split2_test_f2  split3_test_f2  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        0.291580        0.280012        0.226244        0.268817   \n",
       "2        0.286653        0.264970        0.225689        0.262369   \n",
       "3        0.108661        0.097792        0.097792        0.104232   \n",
       "4        0.099338        0.093119        0.088552        0.099558   \n",
       "5        0.284820        0.275037        0.226998        0.273461   \n",
       "6        0.000000        0.000000        0.000000        0.000000   \n",
       "7        0.284820        0.275037        0.226998        0.273461   \n",
       "8        0.097792        0.093148        0.091627        0.099558   \n",
       "9        0.099338        0.093119        0.088552        0.099558   \n",
       "\n",
       "   split4_test_f2  split5_test_f2  split6_test_f2  split7_test_f2  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        0.281899        0.258595        0.268978        0.273297   \n",
       "2        0.272321        0.248046        0.258827        0.263789   \n",
       "3        0.119197        0.096275        0.112888        0.133020   \n",
       "4        0.108627        0.090076        0.100787        0.120879   \n",
       "5        0.274889        0.260948        0.268336        0.274955   \n",
       "6        0.000000        0.000000        0.000000        0.000000   \n",
       "7        0.274889        0.260948        0.268336        0.274955   \n",
       "8        0.111670        0.090104        0.105412        0.122449   \n",
       "9        0.108627        0.090076        0.100787        0.120879   \n",
       "\n",
       "   split8_test_f2  split9_test_f2  mean_test_f2  std_test_f2  rank_test_f2  \\\n",
       "0        0.000000        0.000000      0.000000     0.000000             9   \n",
       "1        0.269461        0.256834      0.267572     0.016971             1   \n",
       "2        0.271446        0.246988      0.260110     0.015901             4   \n",
       "3        0.099432        0.090133      0.105942     0.012191             5   \n",
       "4        0.093295        0.079340      0.097357     0.010911             7   \n",
       "5        0.272374        0.255255      0.266707     0.015343             2   \n",
       "6        0.000000        0.000000      0.000000     0.000000             9   \n",
       "7        0.272374        0.255255      0.266707     0.015343             2   \n",
       "8        0.094847        0.080901      0.098751     0.011248             6   \n",
       "9        0.093295        0.079340      0.097357     0.010911             7   \n",
       "\n",
       "   split0_test_f1  split1_test_f1  split2_test_f1  split3_test_f1  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        0.375120        0.361191        0.300300        0.348837   \n",
       "2        0.371733        0.345703        0.302538        0.343474   \n",
       "3        0.160652        0.145199        0.145199        0.155294   \n",
       "4        0.147368        0.138498        0.132388        0.148585   \n",
       "5        0.368370        0.357629        0.303644        0.355340   \n",
       "6        0.000000        0.000000        0.000000        0.000000   \n",
       "7        0.368370        0.357629        0.303644        0.355340   \n",
       "8        0.145199        0.138660        0.136631        0.148585   \n",
       "9        0.147368        0.138498        0.132388        0.148585   \n",
       "\n",
       "   split4_test_f1  split5_test_f1  split6_test_f1  split7_test_f1  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        0.360531        0.336249        0.349515        0.354651   \n",
       "2        0.350575        0.326733        0.337232        0.345098   \n",
       "3        0.174312        0.143192        0.164948        0.193402   \n",
       "4        0.160465        0.134434        0.149010        0.177215   \n",
       "5        0.352717        0.341847        0.346821        0.357282   \n",
       "6        0.000000        0.000000        0.000000        0.000000   \n",
       "7        0.352717        0.341847        0.346821        0.357282   \n",
       "8        0.164542        0.134593        0.155452        0.179517   \n",
       "9        0.160465        0.134434        0.149010        0.177215   \n",
       "\n",
       "   split8_test_f1  split9_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \\\n",
       "0        0.000000        0.000000      0.000000     0.000000             9   \n",
       "1        0.351562        0.337611      0.347557     0.019094             1   \n",
       "2        0.355599        0.326693      0.340538     0.017872             4   \n",
       "3        0.147887        0.134752      0.156484     0.016579             5   \n",
       "4        0.139480        0.119760      0.144720     0.015137             7   \n",
       "5        0.355122        0.335306      0.347408     0.016985             2   \n",
       "6        0.000000        0.000000      0.000000     0.000000             9   \n",
       "7        0.355122        0.335306      0.347408     0.016985             2   \n",
       "8        0.141677        0.122010      0.146687     0.015573             6   \n",
       "9        0.139480        0.119760      0.144720     0.015137             7   \n",
       "\n",
       "   split0_test_accuracy  split1_test_accuracy  split2_test_accuracy  \\\n",
       "0              0.925926              0.925926              0.925926   \n",
       "1              0.937344              0.936193              0.932930   \n",
       "2              0.937728              0.935713              0.934082   \n",
       "3              0.930819              0.929956              0.929956   \n",
       "4              0.930052              0.929572              0.929572   \n",
       "5              0.937152              0.936576              0.933986   \n",
       "6              0.925926              0.925926              0.925926   \n",
       "7              0.937152              0.936576              0.933986   \n",
       "8              0.929956              0.929668              0.929668   \n",
       "9              0.930052              0.929572              0.929572   \n",
       "\n",
       "   split3_test_accuracy  split4_test_accuracy  split5_test_accuracy  \\\n",
       "0              0.925926              0.925926              0.925926   \n",
       "1              0.935521              0.935329              0.934466   \n",
       "2              0.935809              0.934945              0.934753   \n",
       "3              0.931107              0.930915              0.929956   \n",
       "4              0.930723              0.930723              0.929572   \n",
       "5              0.936289              0.934849              0.935713   \n",
       "6              0.925926              0.925926              0.925926   \n",
       "7              0.936289              0.934849              0.935713   \n",
       "8              0.930723              0.930819              0.929668   \n",
       "9              0.930723              0.930723              0.929572   \n",
       "\n",
       "   split6_test_accuracy  split7_test_accuracy  split8_test_accuracy  \\\n",
       "0              0.925926              0.925926              0.925926   \n",
       "1              0.935713              0.936097              0.936289   \n",
       "2              0.934753              0.935905              0.937056   \n",
       "3              0.930052              0.931971              0.930340   \n",
       "4              0.929860              0.931395              0.930148   \n",
       "5              0.934945              0.936481              0.936576   \n",
       "6              0.925926              0.925926              0.925926   \n",
       "7              0.934945              0.936481              0.936576   \n",
       "8              0.930148              0.931587              0.930244   \n",
       "9              0.929860              0.931395              0.930148   \n",
       "\n",
       "   split9_test_accuracy  mean_test_accuracy  std_test_accuracy  \\\n",
       "0              0.925926            0.925926           0.000000   \n",
       "1              0.935617            0.935550           0.001122   \n",
       "2              0.935137            0.935588           0.001057   \n",
       "3              0.929764            0.930484           0.000668   \n",
       "4              0.929476            0.930109           0.000612   \n",
       "5              0.935329            0.935790           0.000943   \n",
       "6              0.925926            0.925926           0.000000   \n",
       "7              0.935329            0.935790           0.000943   \n",
       "8              0.929572            0.930205           0.000624   \n",
       "9              0.929476            0.930109           0.000612   \n",
       "\n",
       "   rank_test_accuracy  split0_test_balanced_accuracy  \\\n",
       "0                   9                       0.500000   \n",
       "1                   4                       0.622953   \n",
       "2                   3                       0.620777   \n",
       "3                   5                       0.543756   \n",
       "4                   7                       0.539767   \n",
       "5                   1                       0.619870   \n",
       "6                   9                       0.500000   \n",
       "7                   1                       0.619870   \n",
       "8                   6                       0.539119   \n",
       "9                   7                       0.539767   \n",
       "\n",
       "   split1_test_balanced_accuracy  split2_test_balanced_accuracy  \\\n",
       "0                       0.500000                       0.500000   \n",
       "1                       0.617565                       0.593161   \n",
       "2                       0.610751                       0.593187   \n",
       "3                       0.539119                       0.539119   \n",
       "4                       0.537124                       0.535337   \n",
       "5                       0.615389                       0.593731   \n",
       "6                       0.500000                       0.500000   \n",
       "7                       0.615389                       0.593731   \n",
       "8                       0.537176                       0.536580   \n",
       "9                       0.537124                       0.535337   \n",
       "\n",
       "   split3_test_balanced_accuracy  split4_test_balanced_accuracy  \\\n",
       "0                       0.500000                       0.500000   \n",
       "1                       0.612435                       0.618290   \n",
       "2                       0.609611                       0.613912   \n",
       "3                       0.542124                       0.547979   \n",
       "4                       0.540130                       0.543705   \n",
       "5                       0.614637                       0.615052   \n",
       "6                       0.500000                       0.500000   \n",
       "7                       0.614637                       0.615052   \n",
       "8                       0.540130                       0.544948   \n",
       "9                       0.540130                       0.543705   \n",
       "\n",
       "   split5_test_balanced_accuracy  split6_test_balanced_accuracy  \\\n",
       "0                       0.500000                       0.500000   \n",
       "1                       0.607694                       0.612539   \n",
       "2                       0.603083                       0.607850   \n",
       "3                       0.538523                       0.545130   \n",
       "4                       0.535933                       0.540259   \n",
       "5                       0.608964                       0.612124   \n",
       "6                       0.500000                       0.500000   \n",
       "7                       0.608964                       0.612124   \n",
       "8                       0.535984                       0.542202   \n",
       "9                       0.535933                       0.540259   \n",
       "\n",
       "   split7_test_balanced_accuracy  split8_test_balanced_accuracy  \\\n",
       "0                       0.500000                       0.500000   \n",
       "1                       0.614534                       0.612850   \n",
       "2                       0.610259                       0.613860   \n",
       "3                       0.553912                       0.539922   \n",
       "4                       0.548834                       0.537435   \n",
       "5                       0.615337                       0.614197   \n",
       "6                       0.500000                       0.500000   \n",
       "7                       0.615337                       0.614197   \n",
       "8                       0.549534                       0.538083   \n",
       "9                       0.548834                       0.537435   \n",
       "\n",
       "   split9_test_balanced_accuracy  mean_test_balanced_accuracy  \\\n",
       "0                       0.500000                     0.500000   \n",
       "1                       0.607124                     0.611915   \n",
       "2                       0.602694                     0.608598   \n",
       "3                       0.536036                     0.542562   \n",
       "4                       0.531710                     0.539023   \n",
       "5                       0.606373                     0.611567   \n",
       "6                       0.500000                     0.500000   \n",
       "7                       0.606373                     0.611567   \n",
       "8                       0.532358                     0.539611   \n",
       "9                       0.531710                     0.539023   \n",
       "\n",
       "   std_test_balanced_accuracy  rank_test_balanced_accuracy  \\\n",
       "0                    0.000000                            9   \n",
       "1                    0.007716                            1   \n",
       "2                    0.007183                            4   \n",
       "3                    0.005059                            5   \n",
       "4                    0.004519                            7   \n",
       "5                    0.006917                            2   \n",
       "6                    0.000000                            9   \n",
       "7                    0.006917                            2   \n",
       "8                    0.004662                            6   \n",
       "9                    0.004519                            7   \n",
       "\n",
       "   split0_test_precision  split1_test_precision  split2_test_precision  \\\n",
       "0               0.000000               0.000000               0.000000   \n",
       "1               0.717949               0.698885               0.660793   \n",
       "2               0.735632               0.702381               0.699531   \n",
       "3               0.793103               0.756098               0.756098   \n",
       "4               0.759036               0.737500               0.756757   \n",
       "5               0.720755               0.715953               0.694444   \n",
       "6               0.000000               0.000000               0.000000   \n",
       "7               0.720755               0.715953               0.694444   \n",
       "8               0.756098               0.746835               0.753247   \n",
       "9               0.759036               0.737500               0.756757   \n",
       "\n",
       "   split3_test_precision  split4_test_precision  split5_test_precision  \\\n",
       "0               0.000000               0.000000               0.000000   \n",
       "1               0.692308               0.673759               0.673152   \n",
       "2               0.708502               0.672794               0.693277   \n",
       "3               0.846154               0.760000               0.762500   \n",
       "4               0.828947               0.784091               0.750000   \n",
       "5               0.709302               0.667870               0.707317   \n",
       "6               0.000000               0.000000               0.000000   \n",
       "7               0.709302               0.667870               0.707317   \n",
       "8               0.828947               0.780220               0.760000   \n",
       "9               0.828947               0.784091               0.750000   \n",
       "\n",
       "   split6_test_precision  split7_test_precision  split8_test_precision  \\\n",
       "0               0.000000               0.000000               0.000000   \n",
       "1               0.697674               0.703846               0.714286   \n",
       "2               0.681102               0.709677               0.735772   \n",
       "3               0.712871               0.794393               0.787500   \n",
       "4               0.735632               0.793814               0.797297   \n",
       "5               0.676692               0.713178               0.719368   \n",
       "6               0.000000               0.000000               0.000000   \n",
       "7               0.676692               0.713178               0.719368   \n",
       "8               0.744444               0.804124               0.800000   \n",
       "9               0.735632               0.793814               0.797297   \n",
       "\n",
       "   split9_test_precision  mean_test_precision  std_test_precision  \\\n",
       "0               0.000000             0.000000            0.000000   \n",
       "1               0.709544             0.694219            0.018195   \n",
       "2               0.706897             0.704557            0.019245   \n",
       "3               0.770270             0.773899            0.032998   \n",
       "4               0.793651             0.773673            0.028884   \n",
       "5               0.702479             0.702736            0.017064   \n",
       "6               0.000000             0.000000            0.000000   \n",
       "7               0.702479             0.702736            0.017064   \n",
       "8               0.796875             0.777079            0.027620   \n",
       "9               0.793651             0.773673            0.028884   \n",
       "\n",
       "   rank_test_precision  split0_test_recall  split1_test_recall  \\\n",
       "0                    9            0.000000            0.000000   \n",
       "1                    8            0.253886            0.243523   \n",
       "2                    5            0.248705            0.229275   \n",
       "3                    2            0.089378            0.080311   \n",
       "4                    3            0.081606            0.076425   \n",
       "5                    6            0.247409            0.238342   \n",
       "6                    9            0.000000            0.000000   \n",
       "7                    6            0.247409            0.238342   \n",
       "8                    1            0.080311            0.076425   \n",
       "9                    3            0.081606            0.076425   \n",
       "\n",
       "   split2_test_recall  split3_test_recall  split4_test_recall  \\\n",
       "0            0.000000            0.000000            0.000000   \n",
       "1            0.194301            0.233161            0.246114   \n",
       "2            0.193005            0.226684            0.237047   \n",
       "3            0.080311            0.085492            0.098446   \n",
       "4            0.072539            0.081606            0.089378   \n",
       "5            0.194301            0.237047            0.239637   \n",
       "6            0.000000            0.000000            0.000000   \n",
       "7            0.194301            0.237047            0.239637   \n",
       "8            0.075130            0.081606            0.091969   \n",
       "9            0.072539            0.081606            0.089378   \n",
       "\n",
       "   split5_test_recall  split6_test_recall  split7_test_recall  \\\n",
       "0            0.000000            0.000000            0.000000   \n",
       "1            0.224093            0.233161            0.237047   \n",
       "2            0.213731            0.224093            0.227979   \n",
       "3            0.079016            0.093264            0.110104   \n",
       "4            0.073834            0.082902            0.099741   \n",
       "5            0.225389            0.233161            0.238342   \n",
       "6            0.000000            0.000000            0.000000   \n",
       "7            0.225389            0.233161            0.238342   \n",
       "8            0.073834            0.086788            0.101036   \n",
       "9            0.073834            0.082902            0.099741   \n",
       "\n",
       "   split8_test_recall  split9_test_recall  mean_test_recall  std_test_recall  \\\n",
       "0            0.000000            0.000000          0.000000         0.000000   \n",
       "1            0.233161            0.221503          0.231995         0.015603   \n",
       "2            0.234456            0.212435          0.224741         0.014612   \n",
       "3            0.081606            0.073834          0.087176         0.010331   \n",
       "4            0.076425            0.064767          0.079922         0.009179   \n",
       "5            0.235751            0.220207          0.230959         0.014178   \n",
       "6            0.000000            0.000000          0.000000         0.000000   \n",
       "7            0.235751            0.220207          0.230959         0.014178   \n",
       "8            0.077720            0.066062          0.081088         0.009469   \n",
       "9            0.076425            0.064767          0.079922         0.009179   \n",
       "\n",
       "   rank_test_recall  split0_test_roc_auc  split1_test_roc_auc  \\\n",
       "0                 9             0.859600             0.870019   \n",
       "1                 1             0.890980             0.898645   \n",
       "2                 4             0.890408             0.897350   \n",
       "3                 5             0.877668             0.886890   \n",
       "4                 7             0.877155             0.886437   \n",
       "5                 2             0.890648             0.898003   \n",
       "6                 9             0.860678             0.872161   \n",
       "7                 2             0.890648             0.898003   \n",
       "8                 6             0.877277             0.886623   \n",
       "9                 7             0.877155             0.886437   \n",
       "\n",
       "   split2_test_roc_auc  split3_test_roc_auc  split4_test_roc_auc  \\\n",
       "0             0.857176             0.863115             0.861620   \n",
       "1             0.884923             0.891760             0.892111   \n",
       "2             0.885495             0.891843             0.891794   \n",
       "3             0.872804             0.879066             0.880288   \n",
       "4             0.872164             0.879435             0.879533   \n",
       "5             0.884790             0.891769             0.893373   \n",
       "6             0.858087             0.864011             0.862688   \n",
       "7             0.884790             0.891769             0.893373   \n",
       "8             0.872615             0.879823             0.879883   \n",
       "9             0.872164             0.879435             0.879533   \n",
       "\n",
       "   split5_test_roc_auc  split6_test_roc_auc  split7_test_roc_auc  \\\n",
       "0             0.859452             0.868315             0.864777   \n",
       "1             0.889440             0.893558             0.895551   \n",
       "2             0.887883             0.894213             0.894413   \n",
       "3             0.876951             0.884530             0.883032   \n",
       "4             0.876676             0.885140             0.882417   \n",
       "5             0.888339             0.892263             0.895103   \n",
       "6             0.860546             0.869442             0.865066   \n",
       "7             0.888339             0.892263             0.895103   \n",
       "8             0.877626             0.885298             0.882565   \n",
       "9             0.876676             0.885140             0.882417   \n",
       "\n",
       "   split8_test_roc_auc  split9_test_roc_auc  mean_test_roc_auc  \\\n",
       "0             0.860631             0.861574           0.862628   \n",
       "1             0.890260             0.892065           0.891929   \n",
       "2             0.889932             0.890145           0.891347   \n",
       "3             0.878211             0.878437           0.879788   \n",
       "4             0.877532             0.878530           0.879502   \n",
       "5             0.891046             0.889681           0.891502   \n",
       "6             0.861599             0.862464           0.863674   \n",
       "7             0.891046             0.889681           0.891502   \n",
       "8             0.877136             0.878750           0.879760   \n",
       "9             0.877532             0.878530           0.879502   \n",
       "\n",
       "   std_test_roc_auc  rank_test_roc_auc  \n",
       "0          0.003833                 10  \n",
       "1          0.003463                  1  \n",
       "2          0.003235                  4  \n",
       "3          0.003871                  5  \n",
       "4          0.004007                  7  \n",
       "5          0.003445                  2  \n",
       "6          0.004051                  9  \n",
       "7          0.003445                  2  \n",
       "8          0.003941                  6  \n",
       "9          0.004007                  7  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(CV_rs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace02123-653a-46d4-8a73-1cdd8d708cf1",
   "metadata": {},
   "source": [
    "***\n",
    "# Exportamos Dataset y entrenamiento\n",
    "\n",
    "Luego de haber determinado el modelo a utilizar, la configuración de datos óptimo para dicho modelo y sus hiperparámetros, procedo a exportar lo obtenido. El modelo definitivo será entrenado y medido en el siguiente notebook.\n",
    "\n",
    "Además, también importaré el dataset de test y realizaré el recorte de variables que hicimos en train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "06fc7bb5-dc99-431e-81a7-ed3ab4c84274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 30)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fraud_test = pd.read_csv('../data/processed/df_test_ready_to_model.csv')\n",
    "df_fraud_test = df_fraud_test.drop(['source','velocity_24h','velocity_6h','phone_mobile_valid', 'bank_months_count'], axis=1)\n",
    "df_fraud_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f1b91e2a-dbe5-45e1-bb0b-9305ccd32422",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train_2,y_train_2], axis=1).to_csv('../data/processed/df_train_undersamp_ready.csv', index=False)\n",
    "df_fraud_test.to_csv('../data/processed/df_test_cut_ready_to_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "325b0038-b22a-4fac-93c3-35cf3351e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/CV_lgbm.joblib']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(CV_rs, '../models/CV_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1fbfd-e60f-4432-84a6-66c2e94951fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4b101-3fb3-41f8-9bb0-303dba3bc1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b2c0b-fef4-4566-9c45-e00522824693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb4788-db08-4b45-b585-d31c6689f501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practica0",
   "language": "python",
   "name": "practica0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
